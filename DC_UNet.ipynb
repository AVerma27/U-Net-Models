{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DC- UNet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Hcd6WtxhEN3BKIFnBcCv3epyr0Y-utXJ",
      "authorship_tag": "ABX9TyPDRxheQSsJ3zd+YH166nmG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AVerma27/U-Net-Models/blob/main/DC_UNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5xlrVF1wWTJ",
        "outputId": "2c2b00b7-cb8c-4767-baf1-42d73857faaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining Functions"
      ],
      "metadata": {
        "id": "QVO3sKYdj0Oz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from __future__ import print_function\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np \n",
        "import tensorflow as tf\n",
        "import os\n",
        "import glob\n",
        "import skimage.io as io\n",
        "import skimage.transform as trans\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "from tensorflow import reduce_sum\n",
        "from tensorflow.keras.losses import binary_crossentropy"
      ],
      "metadata": {
        "id": "rG4k9742C9Ih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler"
      ],
      "metadata": {
        "id": "GV0TuHhVs0pe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np \n",
        "import os\n",
        "import glob\n",
        "import skimage.io as io\n",
        "import skimage.transform as trans"
      ],
      "metadata": {
        "id": "YTBATXo0DiuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Sky = [128,128,128]\n",
        "Building = [128,0,0]\n",
        "Pole = [192,192,128]\n",
        "Road = [128,64,128]\n",
        "Pavement = [60,40,222]\n",
        "Tree = [128,128,0]\n",
        "SignSymbol = [192,128,128]\n",
        "Fence = [64,64,128]\n",
        "Car = [64,0,128]\n",
        "Pedestrian = [64,64,0]\n",
        "Bicyclist = [0,128,192]\n",
        "Unlabelled = [0,0,0]\n",
        "\n",
        "COLOR_DICT = np.array([Sky, Building, Pole, Road, Pavement,\n",
        "                          Tree, SignSymbol, Fence, Car, Pedestrian, Bicyclist, Unlabelled])\n",
        "\n",
        "\n",
        "def adjustData(img,mask,flag_multi_class,num_class):\n",
        "    if(flag_multi_class):\n",
        "        img = img / 255\n",
        "        mask = mask[:,:,:,0] if(len(mask.shape) == 4) else mask[:,:,0]\n",
        "        new_mask = np.zeros(mask.shape + (num_class,))\n",
        "        for i in range(num_class):\n",
        "            #for one pixel in the image, find the class in mask and convert it into one-hot vector\n",
        "            #index = np.where(mask == i)\n",
        "            #index_mask = (index[0],index[1],index[2],np.zeros(len(index[0]),dtype = np.int64) + i) if (len(mask.shape) == 4) else (index[0],index[1],np.zeros(len(index[0]),dtype = np.int64) + i)\n",
        "            #new_mask[index_mask] = 1\n",
        "            new_mask[mask == i,i] = 1\n",
        "        new_mask = np.reshape(new_mask,(new_mask.shape[0],new_mask.shape[1]*new_mask.shape[2],new_mask.shape[3])) if flag_multi_class else np.reshape(new_mask,(new_mask.shape[0]*new_mask.shape[1],new_mask.shape[2]))\n",
        "        mask = new_mask\n",
        "    elif(np.max(img) > 1):\n",
        "        img = img / 255\n",
        "        mask = mask /255\n",
        "        mask[mask > 0.5] = 1\n",
        "        mask[mask <= 0.5] = 0\n",
        "    return (img,mask)\n",
        "\n",
        "\n",
        "\n",
        "def trainGenerator(batch_size,train_path,image_folder,mask_folder,aug_dict,image_color_mode = \"grayscale\",\n",
        "                    mask_color_mode = \"grayscale\",image_save_prefix  = \"image\",mask_save_prefix  = \"mask\",\n",
        "                    flag_multi_class = False,num_class = 2,save_to_dir = None,target_size = (256,256),seed = 1):\n",
        "    '''\n",
        "    can generate image and mask at the same time\n",
        "    use the same seed for image_datagen and mask_datagen to ensure the transformation for image and mask is the same\n",
        "    if you want to visualize the results of generator, set save_to_dir = \"your path\"\n",
        "    '''\n",
        "    image_datagen = ImageDataGenerator(**aug_dict)\n",
        "    mask_datagen = ImageDataGenerator(**aug_dict)\n",
        "    image_generator = image_datagen.flow_from_directory(\n",
        "        train_path,\n",
        "        classes = [image_folder],\n",
        "        class_mode = None,\n",
        "        color_mode = image_color_mode,\n",
        "        target_size = target_size,\n",
        "        batch_size = batch_size,\n",
        "        save_to_dir = save_to_dir,\n",
        "        save_prefix  = image_save_prefix,\n",
        "        seed = seed)\n",
        "    mask_generator = mask_datagen.flow_from_directory(\n",
        "        train_path,\n",
        "        classes = [mask_folder],\n",
        "        class_mode = None,\n",
        "        color_mode = mask_color_mode,\n",
        "        target_size = target_size,\n",
        "        batch_size = batch_size,\n",
        "        save_to_dir = save_to_dir,\n",
        "        save_prefix  = mask_save_prefix,\n",
        "        seed = seed)\n",
        "    train_generator = zip(image_generator, mask_generator)\n",
        "    for (img,mask) in train_generator:\n",
        "        img,mask = adjustData(img,mask,flag_multi_class,num_class)\n",
        "        yield (img,mask)\n",
        "\n",
        "\n",
        "\n",
        "def testGenerator(test_path,num_image = 30,target_size = (256,256),flag_multi_class = False,as_gray = True):\n",
        "    for i in range(num_image):\n",
        "        img = io.imread(os.path.join(test_path,\"%d.png\"%i),as_gray = as_gray)\n",
        "        img = img / 255\n",
        "        img = trans.resize(img,target_size)\n",
        "        img = np.reshape(img,img.shape+(1,)) if (not flag_multi_class) else img\n",
        "        img = np.reshape(img,(1,)+img.shape)\n",
        "        yield img\n",
        "\n",
        "\n",
        "def geneTrainNpy(image_path,mask_path,flag_multi_class = False,num_class = 2,image_prefix = \"image\",mask_prefix = \"mask\",image_as_gray = True,mask_as_gray = True):\n",
        "    image_name_arr = glob.glob(os.path.join(image_path,\"%s*.png\"%image_prefix))\n",
        "    image_arr = []\n",
        "    mask_arr = []\n",
        "    for index,item in enumerate(image_name_arr):\n",
        "        img = io.imread(item,as_gray = image_as_gray)\n",
        "        img = np.reshape(img,img.shape + (1,)) if image_as_gray else img\n",
        "        mask = io.imread(item.replace(image_path,mask_path).replace(image_prefix,mask_prefix),as_gray = mask_as_gray)\n",
        "        mask = np.reshape(mask,mask.shape + (1,)) if mask_as_gray else mask\n",
        "        img,mask = adjustData(img,mask,flag_multi_class,num_class)\n",
        "        image_arr.append(img)\n",
        "        mask_arr.append(mask)\n",
        "    image_arr = np.array(image_arr)\n",
        "    mask_arr = np.array(mask_arr)\n",
        "    return image_arr,mask_arr\n",
        "\n",
        "\n",
        "def labelVisualize(num_class,color_dict,img):\n",
        "    img = img[:,:,0] if len(img.shape) == 3 else img\n",
        "    img_out = np.zeros(img.shape + (3,))\n",
        "    for i in range(num_class):\n",
        "        img_out[img == i,:] = color_dict[i]\n",
        "    return img_out / 255\n",
        "\n",
        "\n",
        "\n",
        "def saveResult(save_path,npyfile,flag_multi_class = False,num_class = 2):\n",
        "    for i,item in enumerate(npyfile):\n",
        "        img = labelVisualize(num_class,COLOR_DICT,item) if flag_multi_class else item[:,:,0]\n",
        "        io.imsave(os.path.join(save_path,\"%d_predict.png\"%i),img)"
      ],
      "metadata": {
        "id": "8PWSUPTUjr8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation"
      ],
      "metadata": {
        "id": "-zx_Pm71j634"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performing Data Augmentation"
      ],
      "metadata": {
        "id": "TeXOjtovDoVh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_gen_args = dict(rotation_range=0.2,\n",
        "                    width_shift_range=0.05,\n",
        "                    height_shift_range=0.05,\n",
        "                    shear_range=0.05,\n",
        "                    zoom_range=0.05,\n",
        "                    horizontal_flip=True,\n",
        "                    fill_mode='nearest')\n",
        "myGenerator = trainGenerator(20,'/content/drive/MyDrive/membrane - DCUNet/train','image','label',data_gen_args,save_to_dir = \"/content/drive/MyDrive/membrane - DCUNet/train/aug\")"
      ],
      "metadata": {
        "id": "wU6oA9r6j8ao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#you will see 60 transformed images and their masks in data/membrane/train/aug\n",
        "num_batch = 3\n",
        "for i,batch in enumerate(myGenerator):\n",
        "    if(i >= num_batch):\n",
        "        break"
      ],
      "metadata": {
        "id": "PZQsTyPakEhp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9dc1744-598d-44b6-d5f9-1316229bc9f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 30 images belonging to 1 classes.\n",
            "Found 30 images belonging to 1 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "cEfMGEi2kehu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DCUNet"
      ],
      "metadata": {
        "id": "z9r2AQtM-Mym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from keras import initializers\n",
        "from keras.layers import SpatialDropout2D,Input, Conv2D, MaxPooling2D, Conv2DTranspose, concatenate,AveragePooling2D, UpSampling2D, BatchNormalization, Activation, add,Dropout,Permute,ZeroPadding2D,Add, Reshape\n",
        "from keras.models import Model, model_from_json\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.layers.advanced_activations import ELU, LeakyReLU, ReLU, PReLU\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras import backend as K \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from keras import applications, optimizers, callbacks\n",
        "import matplotlib\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras.layers import *"
      ],
      "metadata": {
        "id": "xJ_g1KibDZUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conv2d_bn(x, filters, num_row, num_col, padding='same', strides=(1, 1), activation='relu', name=None):\n",
        "    x = Conv2D(filters, (num_row, num_col), strides=strides, padding=padding, use_bias=False)(x)\n",
        "    x = BatchNormalization(axis=3, scale=False)(x)\n",
        "\n",
        "    if(activation == None):\n",
        "        return x\n",
        "\n",
        "    x = Activation(activation, name=name)(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def trans_conv2d_bn(x, filters, num_row, num_col, padding='same', strides=(2, 2), name=None):\n",
        "    x = Conv2DTranspose(filters, (num_row, num_col), strides=strides, padding=padding)(x)\n",
        "    x = BatchNormalization(axis=3, scale=False)(x)\n",
        "    \n",
        "    return x\n",
        "\n",
        "\n",
        "def DCBlock(U, inp, alpha = 1.67):\n",
        "    W = alpha * U\n",
        "\n",
        "    #shortcut = inp\n",
        "\n",
        "    #shortcut = conv2d_bn(shortcut, int(W*0.167) + int(W*0.333) +\n",
        "    #                      int(W*0.5), 1, 1, activation=None, padding='same')\n",
        "\n",
        "    conv3x3_1 = conv2d_bn(inp, int(W*0.167), 3, 3,\n",
        "                        activation='relu', padding='same')\n",
        "\n",
        "    conv5x5_1 = conv2d_bn(conv3x3_1, int(W*0.333), 3, 3,\n",
        "                        activation='relu', padding='same')\n",
        "\n",
        "    conv7x7_1 = conv2d_bn(conv5x5_1, int(W*0.5), 3, 3,\n",
        "                        activation='relu', padding='same')\n",
        "\n",
        "    out1 = concatenate([conv3x3_1, conv5x5_1, conv7x7_1], axis=3)\n",
        "    out1 = BatchNormalization(axis=3)(out1)\n",
        "    \n",
        "    conv3x3_2 = conv2d_bn(inp, int(W*0.167), 3, 3,\n",
        "                        activation='relu', padding='same')\n",
        "\n",
        "    conv5x5_2 = conv2d_bn(conv3x3_2, int(W*0.333), 3, 3,\n",
        "                        activation='relu', padding='same')\n",
        "\n",
        "    conv7x7_2 = conv2d_bn(conv5x5_2, int(W*0.5), 3, 3,\n",
        "                        activation='relu', padding='same')\n",
        "    out2 = concatenate([conv3x3_2, conv5x5_2, conv7x7_2], axis=3)\n",
        "    out2 = BatchNormalization(axis=3)(out2)\n",
        "\n",
        "    out = add([out1, out2])\n",
        "    out = Activation('relu')(out)\n",
        "    out = BatchNormalization(axis=3)(out)\n",
        "\n",
        "    return out\n",
        "\n",
        "def ResPath(filters, length, inp):\n",
        "    shortcut = inp\n",
        "    shortcut = conv2d_bn(shortcut, filters, 1, 1,\n",
        "                         activation=None, padding='same')\n",
        "\n",
        "    out = conv2d_bn(inp, filters, 3, 3, activation='relu', padding='same')\n",
        "\n",
        "    out = add([shortcut, out])\n",
        "    out = Activation('relu')(out)\n",
        "    out = BatchNormalization(axis=3)(out)\n",
        "\n",
        "    for i in range(length-1):\n",
        "\n",
        "        shortcut = out\n",
        "        shortcut = conv2d_bn(shortcut, filters, 1, 1,\n",
        "                             activation=None, padding='same')\n",
        "\n",
        "        out = conv2d_bn(out, filters, 3, 3, activation='relu', padding='same')\n",
        "\n",
        "        out = add([shortcut, out])\n",
        "        out = Activation('relu')(out)\n",
        "        out = BatchNormalization(axis=3)(out)\n",
        "\n",
        "    return out\n",
        "\n",
        "def DCUNet(pretrained_weights = None,input_size = (256,256,1)):\n",
        "\n",
        "    inputs = Input(input_size)\n",
        "\n",
        "    dcblock1 = DCBlock(32, inputs)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(dcblock1)\n",
        "    dcblock1 = ResPath(32, 4, dcblock1)\n",
        "\n",
        "    dcblock2 = DCBlock(32*2, pool1)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(dcblock2)\n",
        "    dcblock2 = ResPath(32*2, 3, dcblock2)\n",
        "\n",
        "    dcblock3 = DCBlock(32*4, pool2)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(dcblock3)\n",
        "    dcblock3 = ResPath(32*4, 2, dcblock3)\n",
        "\n",
        "    dcblock4 = DCBlock(32*8, pool3)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(dcblock4)\n",
        "    dcblock4 = ResPath(32*8, 1, dcblock4)\n",
        "\n",
        "    dcblock5 = DCBlock(32*16, pool4)\n",
        "\n",
        "    up6 = concatenate([Conv2DTranspose(\n",
        "        32*8, (2, 2), strides=(2, 2), padding='same')(dcblock5), dcblock4], axis=3)\n",
        "    dcblock6 = DCBlock(32*8, up6)\n",
        "\n",
        "    up7 = concatenate([Conv2DTranspose(\n",
        "        32*4, (2, 2), strides=(2, 2), padding='same')(dcblock6), dcblock3], axis=3)\n",
        "    dcblock7 = DCBlock(32*4, up7)\n",
        "\n",
        "    up8 = concatenate([Conv2DTranspose(\n",
        "        32*2, (2, 2), strides=(2, 2), padding='same')(dcblock7), dcblock2], axis=3)\n",
        "    dcblock8 = DCBlock(32*2, up8)\n",
        "\n",
        "    up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(\n",
        "        2, 2), padding='same')(dcblock8), dcblock1], axis=3)\n",
        "    dcblock9 = DCBlock(32, up9)\n",
        "\n",
        "    conv10 = conv2d_bn(dcblock9, 1, 1, 1, activation='sigmoid')\n",
        "    \n",
        "    model = Model(inputs=[inputs], outputs=[conv10])\n",
        "    \n",
        "    return model"
      ],
      "metadata": {
        "id": "RtqxpmwFkKur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Main"
      ],
      "metadata": {
        "id": "G8yLq26sOpl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " \n",
        "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "\n",
        "data_gen_args = dict(rotation_range=0.2,\n",
        "                    width_shift_range=0.05,\n",
        "                    height_shift_range=0.05,\n",
        "                    shear_range=0.05,\n",
        "                    zoom_range=0.05,\n",
        "                    horizontal_flip=True,\n",
        "                    fill_mode='nearest')\n",
        "myGene = trainGenerator(2,\"/content/drive/MyDrive/membrane - DCUNet/train\",'image','label',data_gen_args,save_to_dir = None)\n",
        "\n",
        "model = DCUNet()\n",
        "model_checkpoint = ModelCheckpoint('DCUNet_membrane.hdf5', monitor='loss',verbose=1, save_best_only=True)\n"
      ],
      "metadata": {
        "id": "LAEfEgjfkz2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfX5pt58ElmR",
        "outputId": "86d31a07-c82d-4a8c-a0fd-f82ee8287b14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 256, 256, 1  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 256, 256, 8)  72          ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 256, 256, 8)  72          ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 256, 256, 8)  24         ['conv2d[0][0]']                 \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 256, 256, 8)  24         ['conv2d_3[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 256, 256, 8)  0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 256, 256, 8)  0           ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 256, 256, 17  1224        ['activation[0][0]']             \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 256, 256, 17  1224        ['activation_3[0][0]']           \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 256, 256, 17  51         ['conv2d_1[0][0]']               \n",
            " rmalization)                   )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 256, 256, 17  51         ['conv2d_4[0][0]']               \n",
            " rmalization)                   )                                                                 \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 256, 256, 17  0           ['batch_normalization_1[0][0]']  \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 256, 256, 17  0           ['batch_normalization_5[0][0]']  \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 256, 256, 26  3978        ['activation_1[0][0]']           \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 256, 256, 26  3978        ['activation_4[0][0]']           \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 256, 256, 26  78         ['conv2d_2[0][0]']               \n",
            " rmalization)                   )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 256, 256, 26  78         ['conv2d_5[0][0]']               \n",
            " rmalization)                   )                                                                 \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 256, 256, 26  0           ['batch_normalization_2[0][0]']  \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 256, 256, 26  0           ['batch_normalization_6[0][0]']  \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 256, 256, 51  0           ['activation[0][0]',             \n",
            "                                )                                 'activation_1[0][0]',           \n",
            "                                                                  'activation_2[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 256, 256, 51  0           ['activation_3[0][0]',           \n",
            "                                )                                 'activation_4[0][0]',           \n",
            "                                                                  'activation_5[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 256, 256, 51  204        ['concatenate[0][0]']            \n",
            " rmalization)                   )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 256, 256, 51  204        ['concatenate_1[0][0]']          \n",
            " rmalization)                   )                                                                 \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 256, 256, 51  0           ['batch_normalization_3[0][0]',  \n",
            "                                )                                 'batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 256, 256, 51  0           ['add[0][0]']                    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 256, 256, 51  204        ['activation_6[0][0]']           \n",
            " rmalization)                   )                                                                 \n",
            "                                                                                                  \n",
            " max_pooling2d (MaxPooling2D)   (None, 128, 128, 51  0           ['batch_normalization_8[0][0]']  \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 128, 128, 17  7803        ['max_pooling2d[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)             (None, 128, 128, 17  7803        ['max_pooling2d[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_21 (BatchN  (None, 128, 128, 17  51         ['conv2d_14[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_25 (BatchN  (None, 128, 128, 17  51         ['conv2d_17[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " activation_15 (Activation)     (None, 128, 128, 17  0           ['batch_normalization_21[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " activation_18 (Activation)     (None, 128, 128, 17  0           ['batch_normalization_25[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, 128, 128, 35  5355        ['activation_15[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)             (None, 128, 128, 35  5355        ['activation_18[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_22 (BatchN  (None, 128, 128, 35  105        ['conv2d_15[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_26 (BatchN  (None, 128, 128, 35  105        ['conv2d_18[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " activation_16 (Activation)     (None, 128, 128, 35  0           ['batch_normalization_22[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " activation_19 (Activation)     (None, 128, 128, 35  0           ['batch_normalization_26[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)             (None, 128, 128, 53  16695       ['activation_16[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)             (None, 128, 128, 53  16695       ['activation_19[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_23 (BatchN  (None, 128, 128, 53  159        ['conv2d_16[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_27 (BatchN  (None, 128, 128, 53  159        ['conv2d_19[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " activation_17 (Activation)     (None, 128, 128, 53  0           ['batch_normalization_23[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " activation_20 (Activation)     (None, 128, 128, 53  0           ['batch_normalization_27[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate)    (None, 128, 128, 10  0           ['activation_15[0][0]',          \n",
            "                                5)                                'activation_16[0][0]',          \n",
            "                                                                  'activation_17[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_3 (Concatenate)    (None, 128, 128, 10  0           ['activation_18[0][0]',          \n",
            "                                5)                                'activation_19[0][0]',          \n",
            "                                                                  'activation_20[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_24 (BatchN  (None, 128, 128, 10  420        ['concatenate_2[0][0]']          \n",
            " ormalization)                  5)                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_28 (BatchN  (None, 128, 128, 10  420        ['concatenate_3[0][0]']          \n",
            " ormalization)                  5)                                                                \n",
            "                                                                                                  \n",
            " add_5 (Add)                    (None, 128, 128, 10  0           ['batch_normalization_24[0][0]', \n",
            "                                5)                                'batch_normalization_28[0][0]'] \n",
            "                                                                                                  \n",
            " activation_21 (Activation)     (None, 128, 128, 10  0           ['add_5[0][0]']                  \n",
            "                                5)                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_29 (BatchN  (None, 128, 128, 10  420        ['activation_21[0][0]']          \n",
            " ormalization)                  5)                                                                \n",
            "                                                                                                  \n",
            " max_pooling2d_1 (MaxPooling2D)  (None, 64, 64, 105)  0          ['batch_normalization_29[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_26 (Conv2D)             (None, 64, 64, 35)   33075       ['max_pooling2d_1[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_29 (Conv2D)             (None, 64, 64, 35)   33075       ['max_pooling2d_1[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_39 (BatchN  (None, 64, 64, 35)  105         ['conv2d_26[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_43 (BatchN  (None, 64, 64, 35)  105         ['conv2d_29[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_28 (Activation)     (None, 64, 64, 35)   0           ['batch_normalization_39[0][0]'] \n",
            "                                                                                                  \n",
            " activation_31 (Activation)     (None, 64, 64, 35)   0           ['batch_normalization_43[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_27 (Conv2D)             (None, 64, 64, 71)   22365       ['activation_28[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_30 (Conv2D)             (None, 64, 64, 71)   22365       ['activation_31[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_40 (BatchN  (None, 64, 64, 71)  213         ['conv2d_27[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_44 (BatchN  (None, 64, 64, 71)  213         ['conv2d_30[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_29 (Activation)     (None, 64, 64, 71)   0           ['batch_normalization_40[0][0]'] \n",
            "                                                                                                  \n",
            " activation_32 (Activation)     (None, 64, 64, 71)   0           ['batch_normalization_44[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_28 (Conv2D)             (None, 64, 64, 106)  67734       ['activation_29[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_31 (Conv2D)             (None, 64, 64, 106)  67734       ['activation_32[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_41 (BatchN  (None, 64, 64, 106)  318        ['conv2d_28[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_45 (BatchN  (None, 64, 64, 106)  318        ['conv2d_31[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_30 (Activation)     (None, 64, 64, 106)  0           ['batch_normalization_41[0][0]'] \n",
            "                                                                                                  \n",
            " activation_33 (Activation)     (None, 64, 64, 106)  0           ['batch_normalization_45[0][0]'] \n",
            "                                                                                                  \n",
            " concatenate_4 (Concatenate)    (None, 64, 64, 212)  0           ['activation_28[0][0]',          \n",
            "                                                                  'activation_29[0][0]',          \n",
            "                                                                  'activation_30[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_5 (Concatenate)    (None, 64, 64, 212)  0           ['activation_31[0][0]',          \n",
            "                                                                  'activation_32[0][0]',          \n",
            "                                                                  'activation_33[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_42 (BatchN  (None, 64, 64, 212)  848        ['concatenate_4[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_46 (BatchN  (None, 64, 64, 212)  848        ['concatenate_5[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_9 (Add)                    (None, 64, 64, 212)  0           ['batch_normalization_42[0][0]', \n",
            "                                                                  'batch_normalization_46[0][0]'] \n",
            "                                                                                                  \n",
            " activation_34 (Activation)     (None, 64, 64, 212)  0           ['add_9[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_47 (BatchN  (None, 64, 64, 212)  848        ['activation_34[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " max_pooling2d_2 (MaxPooling2D)  (None, 32, 32, 212)  0          ['batch_normalization_47[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_36 (Conv2D)             (None, 32, 32, 71)   135468      ['max_pooling2d_2[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_39 (Conv2D)             (None, 32, 32, 71)   135468      ['max_pooling2d_2[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_54 (BatchN  (None, 32, 32, 71)  213         ['conv2d_36[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_58 (BatchN  (None, 32, 32, 71)  213         ['conv2d_39[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_39 (Activation)     (None, 32, 32, 71)   0           ['batch_normalization_54[0][0]'] \n",
            "                                                                                                  \n",
            " activation_42 (Activation)     (None, 32, 32, 71)   0           ['batch_normalization_58[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_37 (Conv2D)             (None, 32, 32, 142)  90738       ['activation_39[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_40 (Conv2D)             (None, 32, 32, 142)  90738       ['activation_42[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_55 (BatchN  (None, 32, 32, 142)  426        ['conv2d_37[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_59 (BatchN  (None, 32, 32, 142)  426        ['conv2d_40[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_40 (Activation)     (None, 32, 32, 142)  0           ['batch_normalization_55[0][0]'] \n",
            "                                                                                                  \n",
            " activation_43 (Activation)     (None, 32, 32, 142)  0           ['batch_normalization_59[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_38 (Conv2D)             (None, 32, 32, 213)  272214      ['activation_40[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_41 (Conv2D)             (None, 32, 32, 213)  272214      ['activation_43[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_56 (BatchN  (None, 32, 32, 213)  639        ['conv2d_38[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_60 (BatchN  (None, 32, 32, 213)  639        ['conv2d_41[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_41 (Activation)     (None, 32, 32, 213)  0           ['batch_normalization_56[0][0]'] \n",
            "                                                                                                  \n",
            " activation_44 (Activation)     (None, 32, 32, 213)  0           ['batch_normalization_60[0][0]'] \n",
            "                                                                                                  \n",
            " concatenate_6 (Concatenate)    (None, 32, 32, 426)  0           ['activation_39[0][0]',          \n",
            "                                                                  'activation_40[0][0]',          \n",
            "                                                                  'activation_41[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_7 (Concatenate)    (None, 32, 32, 426)  0           ['activation_42[0][0]',          \n",
            "                                                                  'activation_43[0][0]',          \n",
            "                                                                  'activation_44[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_57 (BatchN  (None, 32, 32, 426)  1704       ['concatenate_6[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_61 (BatchN  (None, 32, 32, 426)  1704       ['concatenate_7[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_12 (Add)                   (None, 32, 32, 426)  0           ['batch_normalization_57[0][0]', \n",
            "                                                                  'batch_normalization_61[0][0]'] \n",
            "                                                                                                  \n",
            " activation_45 (Activation)     (None, 32, 32, 426)  0           ['add_12[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_62 (BatchN  (None, 32, 32, 426)  1704       ['activation_45[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " max_pooling2d_3 (MaxPooling2D)  (None, 16, 16, 426)  0          ['batch_normalization_62[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_44 (Conv2D)             (None, 16, 16, 142)  544428      ['max_pooling2d_3[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_47 (Conv2D)             (None, 16, 16, 142)  544428      ['max_pooling2d_3[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_66 (BatchN  (None, 16, 16, 142)  426        ['conv2d_44[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_70 (BatchN  (None, 16, 16, 142)  426        ['conv2d_47[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_48 (Activation)     (None, 16, 16, 142)  0           ['batch_normalization_66[0][0]'] \n",
            "                                                                                                  \n",
            " activation_51 (Activation)     (None, 16, 16, 142)  0           ['batch_normalization_70[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_45 (Conv2D)             (None, 16, 16, 284)  362952      ['activation_48[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_48 (Conv2D)             (None, 16, 16, 284)  362952      ['activation_51[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_67 (BatchN  (None, 16, 16, 284)  852        ['conv2d_45[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_71 (BatchN  (None, 16, 16, 284)  852        ['conv2d_48[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_49 (Activation)     (None, 16, 16, 284)  0           ['batch_normalization_67[0][0]'] \n",
            "                                                                                                  \n",
            " activation_52 (Activation)     (None, 16, 16, 284)  0           ['batch_normalization_71[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_46 (Conv2D)             (None, 16, 16, 427)  1091412     ['activation_49[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_49 (Conv2D)             (None, 16, 16, 427)  1091412     ['activation_52[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_68 (BatchN  (None, 16, 16, 427)  1281       ['conv2d_46[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_72 (BatchN  (None, 16, 16, 427)  1281       ['conv2d_49[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_50 (Activation)     (None, 16, 16, 427)  0           ['batch_normalization_68[0][0]'] \n",
            "                                                                                                  \n",
            " activation_53 (Activation)     (None, 16, 16, 427)  0           ['batch_normalization_72[0][0]'] \n",
            "                                                                                                  \n",
            " concatenate_8 (Concatenate)    (None, 16, 16, 853)  0           ['activation_48[0][0]',          \n",
            "                                                                  'activation_49[0][0]',          \n",
            "                                                                  'activation_50[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_9 (Concatenate)    (None, 16, 16, 853)  0           ['activation_51[0][0]',          \n",
            "                                                                  'activation_52[0][0]',          \n",
            "                                                                  'activation_53[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_43 (Conv2D)             (None, 32, 32, 256)  981504      ['batch_normalization_62[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_69 (BatchN  (None, 16, 16, 853)  3412       ['concatenate_8[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_73 (BatchN  (None, 16, 16, 853)  3412       ['concatenate_9[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_42 (Conv2D)             (None, 32, 32, 256)  109056      ['batch_normalization_62[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_64 (BatchN  (None, 32, 32, 256)  768        ['conv2d_43[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_14 (Add)                   (None, 16, 16, 853)  0           ['batch_normalization_69[0][0]', \n",
            "                                                                  'batch_normalization_73[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_63 (BatchN  (None, 32, 32, 256)  768        ['conv2d_42[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_46 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_64[0][0]'] \n",
            "                                                                                                  \n",
            " activation_54 (Activation)     (None, 16, 16, 853)  0           ['add_14[0][0]']                 \n",
            "                                                                                                  \n",
            " add_13 (Add)                   (None, 32, 32, 256)  0           ['batch_normalization_63[0][0]', \n",
            "                                                                  'activation_46[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_74 (BatchN  (None, 16, 16, 853)  3412       ['activation_54[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_47 (Activation)     (None, 32, 32, 256)  0           ['add_13[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_transpose (Conv2DTransp  (None, 32, 32, 256)  873728     ['batch_normalization_74[0][0]'] \n",
            " ose)                                                                                             \n",
            "                                                                                                  \n",
            " batch_normalization_65 (BatchN  (None, 32, 32, 256)  1024       ['activation_47[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " concatenate_10 (Concatenate)   (None, 32, 32, 512)  0           ['conv2d_transpose[0][0]',       \n",
            "                                                                  'batch_normalization_65[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_50 (Conv2D)             (None, 32, 32, 71)   327168      ['concatenate_10[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_53 (Conv2D)             (None, 32, 32, 71)   327168      ['concatenate_10[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_75 (BatchN  (None, 32, 32, 71)  213         ['conv2d_50[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_79 (BatchN  (None, 32, 32, 71)  213         ['conv2d_53[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_55 (Activation)     (None, 32, 32, 71)   0           ['batch_normalization_75[0][0]'] \n",
            "                                                                                                  \n",
            " activation_58 (Activation)     (None, 32, 32, 71)   0           ['batch_normalization_79[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_51 (Conv2D)             (None, 32, 32, 142)  90738       ['activation_55[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_54 (Conv2D)             (None, 32, 32, 142)  90738       ['activation_58[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_33 (Conv2D)             (None, 64, 64, 128)  244224      ['batch_normalization_47[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_76 (BatchN  (None, 32, 32, 142)  426        ['conv2d_51[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_80 (BatchN  (None, 32, 32, 142)  426        ['conv2d_54[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_32 (Conv2D)             (None, 64, 64, 128)  27136       ['batch_normalization_47[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_49 (BatchN  (None, 64, 64, 128)  384        ['conv2d_33[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_56 (Activation)     (None, 32, 32, 142)  0           ['batch_normalization_76[0][0]'] \n",
            "                                                                                                  \n",
            " activation_59 (Activation)     (None, 32, 32, 142)  0           ['batch_normalization_80[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_48 (BatchN  (None, 64, 64, 128)  384        ['conv2d_32[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_35 (Activation)     (None, 64, 64, 128)  0           ['batch_normalization_49[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_52 (Conv2D)             (None, 32, 32, 213)  272214      ['activation_56[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_55 (Conv2D)             (None, 32, 32, 213)  272214      ['activation_59[0][0]']          \n",
            "                                                                                                  \n",
            " add_10 (Add)                   (None, 64, 64, 128)  0           ['batch_normalization_48[0][0]', \n",
            "                                                                  'activation_35[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_77 (BatchN  (None, 32, 32, 213)  639        ['conv2d_52[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_81 (BatchN  (None, 32, 32, 213)  639        ['conv2d_55[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_36 (Activation)     (None, 64, 64, 128)  0           ['add_10[0][0]']                 \n",
            "                                                                                                  \n",
            " activation_57 (Activation)     (None, 32, 32, 213)  0           ['batch_normalization_77[0][0]'] \n",
            "                                                                                                  \n",
            " activation_60 (Activation)     (None, 32, 32, 213)  0           ['batch_normalization_81[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_50 (BatchN  (None, 64, 64, 128)  512        ['activation_36[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " concatenate_11 (Concatenate)   (None, 32, 32, 426)  0           ['activation_55[0][0]',          \n",
            "                                                                  'activation_56[0][0]',          \n",
            "                                                                  'activation_57[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_12 (Concatenate)   (None, 32, 32, 426)  0           ['activation_58[0][0]',          \n",
            "                                                                  'activation_59[0][0]',          \n",
            "                                                                  'activation_60[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_35 (Conv2D)             (None, 64, 64, 128)  147456      ['batch_normalization_50[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_78 (BatchN  (None, 32, 32, 426)  1704       ['concatenate_11[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_82 (BatchN  (None, 32, 32, 426)  1704       ['concatenate_12[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_34 (Conv2D)             (None, 64, 64, 128)  16384       ['batch_normalization_50[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_52 (BatchN  (None, 64, 64, 128)  384        ['conv2d_35[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_15 (Add)                   (None, 32, 32, 426)  0           ['batch_normalization_78[0][0]', \n",
            "                                                                  'batch_normalization_82[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_51 (BatchN  (None, 64, 64, 128)  384        ['conv2d_34[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_37 (Activation)     (None, 64, 64, 128)  0           ['batch_normalization_52[0][0]'] \n",
            "                                                                                                  \n",
            " activation_61 (Activation)     (None, 32, 32, 426)  0           ['add_15[0][0]']                 \n",
            "                                                                                                  \n",
            " add_11 (Add)                   (None, 64, 64, 128)  0           ['batch_normalization_51[0][0]', \n",
            "                                                                  'activation_37[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_83 (BatchN  (None, 32, 32, 426)  1704       ['activation_61[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_38 (Activation)     (None, 64, 64, 128)  0           ['add_11[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)             (None, 128, 128, 64  60480       ['batch_normalization_29[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_transpose_1 (Conv2DTran  (None, 64, 64, 128)  218240     ['batch_normalization_83[0][0]'] \n",
            " spose)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_53 (BatchN  (None, 64, 64, 128)  512        ['activation_38[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)             (None, 128, 128, 64  6720        ['batch_normalization_29[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_31 (BatchN  (None, 128, 128, 64  192        ['conv2d_21[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " concatenate_13 (Concatenate)   (None, 64, 64, 256)  0           ['conv2d_transpose_1[0][0]',     \n",
            "                                                                  'batch_normalization_53[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_30 (BatchN  (None, 128, 128, 64  192        ['conv2d_20[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " activation_22 (Activation)     (None, 128, 128, 64  0           ['batch_normalization_31[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_56 (Conv2D)             (None, 64, 64, 35)   80640       ['concatenate_13[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_59 (Conv2D)             (None, 64, 64, 35)   80640       ['concatenate_13[0][0]']         \n",
            "                                                                                                  \n",
            " add_6 (Add)                    (None, 128, 128, 64  0           ['batch_normalization_30[0][0]', \n",
            "                                )                                 'activation_22[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_84 (BatchN  (None, 64, 64, 35)  105         ['conv2d_56[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_88 (BatchN  (None, 64, 64, 35)  105         ['conv2d_59[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_23 (Activation)     (None, 128, 128, 64  0           ['add_6[0][0]']                  \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " activation_62 (Activation)     (None, 64, 64, 35)   0           ['batch_normalization_84[0][0]'] \n",
            "                                                                                                  \n",
            " activation_65 (Activation)     (None, 64, 64, 35)   0           ['batch_normalization_88[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_32 (BatchN  (None, 128, 128, 64  256        ['activation_23[0][0]']          \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_57 (Conv2D)             (None, 64, 64, 71)   22365       ['activation_62[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_60 (Conv2D)             (None, 64, 64, 71)   22365       ['activation_65[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_23 (Conv2D)             (None, 128, 128, 64  36864       ['batch_normalization_32[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_85 (BatchN  (None, 64, 64, 71)  213         ['conv2d_57[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_89 (BatchN  (None, 64, 64, 71)  213         ['conv2d_60[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_22 (Conv2D)             (None, 128, 128, 64  4096        ['batch_normalization_32[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_34 (BatchN  (None, 128, 128, 64  192        ['conv2d_23[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " activation_63 (Activation)     (None, 64, 64, 71)   0           ['batch_normalization_85[0][0]'] \n",
            "                                                                                                  \n",
            " activation_66 (Activation)     (None, 64, 64, 71)   0           ['batch_normalization_89[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_33 (BatchN  (None, 128, 128, 64  192        ['conv2d_22[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " activation_24 (Activation)     (None, 128, 128, 64  0           ['batch_normalization_34[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_58 (Conv2D)             (None, 64, 64, 106)  67734       ['activation_63[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_61 (Conv2D)             (None, 64, 64, 106)  67734       ['activation_66[0][0]']          \n",
            "                                                                                                  \n",
            " add_7 (Add)                    (None, 128, 128, 64  0           ['batch_normalization_33[0][0]', \n",
            "                                )                                 'activation_24[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_86 (BatchN  (None, 64, 64, 106)  318        ['conv2d_58[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_90 (BatchN  (None, 64, 64, 106)  318        ['conv2d_61[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_25 (Activation)     (None, 128, 128, 64  0           ['add_7[0][0]']                  \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 256, 256, 32  14688       ['batch_normalization_8[0][0]']  \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " activation_64 (Activation)     (None, 64, 64, 106)  0           ['batch_normalization_86[0][0]'] \n",
            "                                                                                                  \n",
            " activation_67 (Activation)     (None, 64, 64, 106)  0           ['batch_normalization_90[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_35 (BatchN  (None, 128, 128, 64  256        ['activation_25[0][0]']          \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 256, 256, 32  1632        ['batch_normalization_8[0][0]']  \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 256, 256, 32  96         ['conv2d_7[0][0]']               \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " concatenate_14 (Concatenate)   (None, 64, 64, 212)  0           ['activation_62[0][0]',          \n",
            "                                                                  'activation_63[0][0]',          \n",
            "                                                                  'activation_64[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_15 (Concatenate)   (None, 64, 64, 212)  0           ['activation_65[0][0]',          \n",
            "                                                                  'activation_66[0][0]',          \n",
            "                                                                  'activation_67[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_25 (Conv2D)             (None, 128, 128, 64  36864       ['batch_normalization_35[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 256, 256, 32  96         ['conv2d_6[0][0]']               \n",
            " rmalization)                   )                                                                 \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 256, 256, 32  0           ['batch_normalization_10[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_87 (BatchN  (None, 64, 64, 212)  848        ['concatenate_14[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_91 (BatchN  (None, 64, 64, 212)  848        ['concatenate_15[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_24 (Conv2D)             (None, 128, 128, 64  4096        ['batch_normalization_35[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_37 (BatchN  (None, 128, 128, 64  192        ['conv2d_25[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 256, 256, 32  0           ['batch_normalization_9[0][0]',  \n",
            "                                )                                 'activation_7[0][0]']           \n",
            "                                                                                                  \n",
            " add_16 (Add)                   (None, 64, 64, 212)  0           ['batch_normalization_87[0][0]', \n",
            "                                                                  'batch_normalization_91[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_36 (BatchN  (None, 128, 128, 64  192        ['conv2d_24[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " activation_26 (Activation)     (None, 128, 128, 64  0           ['batch_normalization_37[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 256, 256, 32  0           ['add_1[0][0]']                  \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " activation_68 (Activation)     (None, 64, 64, 212)  0           ['add_16[0][0]']                 \n",
            "                                                                                                  \n",
            " add_8 (Add)                    (None, 128, 128, 64  0           ['batch_normalization_36[0][0]', \n",
            "                                )                                 'activation_26[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 256, 256, 32  128        ['activation_8[0][0]']           \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_92 (BatchN  (None, 64, 64, 212)  848        ['activation_68[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_27 (Activation)     (None, 128, 128, 64  0           ['add_8[0][0]']                  \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 256, 256, 32  9216        ['batch_normalization_11[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_transpose_2 (Conv2DTran  (None, 128, 128, 64  54336      ['batch_normalization_92[0][0]'] \n",
            " spose)                         )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_38 (BatchN  (None, 128, 128, 64  256        ['activation_27[0][0]']          \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 256, 256, 32  1024        ['batch_normalization_11[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 256, 256, 32  96         ['conv2d_9[0][0]']               \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " concatenate_16 (Concatenate)   (None, 128, 128, 12  0           ['conv2d_transpose_2[0][0]',     \n",
            "                                8)                                'batch_normalization_38[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 256, 256, 32  96         ['conv2d_8[0][0]']               \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 256, 256, 32  0           ['batch_normalization_13[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_62 (Conv2D)             (None, 128, 128, 17  19584       ['concatenate_16[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_65 (Conv2D)             (None, 128, 128, 17  19584       ['concatenate_16[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 256, 256, 32  0           ['batch_normalization_12[0][0]', \n",
            "                                )                                 'activation_9[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_93 (BatchN  (None, 128, 128, 17  51         ['conv2d_62[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_97 (BatchN  (None, 128, 128, 17  51         ['conv2d_65[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 256, 256, 32  0           ['add_2[0][0]']                  \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " activation_69 (Activation)     (None, 128, 128, 17  0           ['batch_normalization_93[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " activation_72 (Activation)     (None, 128, 128, 17  0           ['batch_normalization_97[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 256, 256, 32  128        ['activation_10[0][0]']          \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_63 (Conv2D)             (None, 128, 128, 35  5355        ['activation_69[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_66 (Conv2D)             (None, 128, 128, 35  5355        ['activation_72[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 256, 256, 32  9216        ['batch_normalization_14[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_94 (BatchN  (None, 128, 128, 35  105        ['conv2d_63[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_98 (BatchN  (None, 128, 128, 35  105        ['conv2d_66[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 256, 256, 32  1024        ['batch_normalization_14[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 256, 256, 32  96         ['conv2d_11[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " activation_70 (Activation)     (None, 128, 128, 35  0           ['batch_normalization_94[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " activation_73 (Activation)     (None, 128, 128, 35  0           ['batch_normalization_98[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 256, 256, 32  96         ['conv2d_10[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 256, 256, 32  0           ['batch_normalization_16[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_64 (Conv2D)             (None, 128, 128, 53  16695       ['activation_70[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_67 (Conv2D)             (None, 128, 128, 53  16695       ['activation_73[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 256, 256, 32  0           ['batch_normalization_15[0][0]', \n",
            "                                )                                 'activation_11[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_95 (BatchN  (None, 128, 128, 53  159        ['conv2d_64[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_99 (BatchN  (None, 128, 128, 53  159        ['conv2d_67[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 256, 256, 32  0           ['add_3[0][0]']                  \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " activation_71 (Activation)     (None, 128, 128, 53  0           ['batch_normalization_95[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " activation_74 (Activation)     (None, 128, 128, 53  0           ['batch_normalization_99[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 256, 256, 32  128        ['activation_12[0][0]']          \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " concatenate_17 (Concatenate)   (None, 128, 128, 10  0           ['activation_69[0][0]',          \n",
            "                                5)                                'activation_70[0][0]',          \n",
            "                                                                  'activation_71[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_18 (Concatenate)   (None, 128, 128, 10  0           ['activation_72[0][0]',          \n",
            "                                5)                                'activation_73[0][0]',          \n",
            "                                                                  'activation_74[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 256, 256, 32  9216        ['batch_normalization_17[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_96 (BatchN  (None, 128, 128, 10  420        ['concatenate_17[0][0]']         \n",
            " ormalization)                  5)                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_100 (Batch  (None, 128, 128, 10  420        ['concatenate_18[0][0]']         \n",
            " Normalization)                 5)                                                                \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 256, 256, 32  1024        ['batch_normalization_17[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_19 (BatchN  (None, 256, 256, 32  96         ['conv2d_13[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " add_17 (Add)                   (None, 128, 128, 10  0           ['batch_normalization_96[0][0]', \n",
            "                                5)                                'batch_normalization_100[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_18 (BatchN  (None, 256, 256, 32  96         ['conv2d_12[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 256, 256, 32  0           ['batch_normalization_19[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " activation_75 (Activation)     (None, 128, 128, 10  0           ['add_17[0][0]']                 \n",
            "                                5)                                                                \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 256, 256, 32  0           ['batch_normalization_18[0][0]', \n",
            "                                )                                 'activation_13[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_101 (Batch  (None, 128, 128, 10  420        ['activation_75[0][0]']          \n",
            " Normalization)                 5)                                                                \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 256, 256, 32  0           ['add_4[0][0]']                  \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_transpose_3 (Conv2DTran  (None, 256, 256, 32  13472      ['batch_normalization_101[0][0]']\n",
            " spose)                         )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_20 (BatchN  (None, 256, 256, 32  128        ['activation_14[0][0]']          \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " concatenate_19 (Concatenate)   (None, 256, 256, 64  0           ['conv2d_transpose_3[0][0]',     \n",
            "                                )                                 'batch_normalization_20[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_68 (Conv2D)             (None, 256, 256, 8)  4608        ['concatenate_19[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_71 (Conv2D)             (None, 256, 256, 8)  4608        ['concatenate_19[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_102 (Batch  (None, 256, 256, 8)  24         ['conv2d_68[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_106 (Batch  (None, 256, 256, 8)  24         ['conv2d_71[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_76 (Activation)     (None, 256, 256, 8)  0           ['batch_normalization_102[0][0]']\n",
            "                                                                                                  \n",
            " activation_79 (Activation)     (None, 256, 256, 8)  0           ['batch_normalization_106[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_69 (Conv2D)             (None, 256, 256, 17  1224        ['activation_76[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_72 (Conv2D)             (None, 256, 256, 17  1224        ['activation_79[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_103 (Batch  (None, 256, 256, 17  51         ['conv2d_69[0][0]']              \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_107 (Batch  (None, 256, 256, 17  51         ['conv2d_72[0][0]']              \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " activation_77 (Activation)     (None, 256, 256, 17  0           ['batch_normalization_103[0][0]']\n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " activation_80 (Activation)     (None, 256, 256, 17  0           ['batch_normalization_107[0][0]']\n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_70 (Conv2D)             (None, 256, 256, 26  3978        ['activation_77[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_73 (Conv2D)             (None, 256, 256, 26  3978        ['activation_80[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_104 (Batch  (None, 256, 256, 26  78         ['conv2d_70[0][0]']              \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_108 (Batch  (None, 256, 256, 26  78         ['conv2d_73[0][0]']              \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " activation_78 (Activation)     (None, 256, 256, 26  0           ['batch_normalization_104[0][0]']\n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " activation_81 (Activation)     (None, 256, 256, 26  0           ['batch_normalization_108[0][0]']\n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " concatenate_20 (Concatenate)   (None, 256, 256, 51  0           ['activation_76[0][0]',          \n",
            "                                )                                 'activation_77[0][0]',          \n",
            "                                                                  'activation_78[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_21 (Concatenate)   (None, 256, 256, 51  0           ['activation_79[0][0]',          \n",
            "                                )                                 'activation_80[0][0]',          \n",
            "                                                                  'activation_81[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_105 (Batch  (None, 256, 256, 51  204        ['concatenate_20[0][0]']         \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_109 (Batch  (None, 256, 256, 51  204        ['concatenate_21[0][0]']         \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " add_18 (Add)                   (None, 256, 256, 51  0           ['batch_normalization_105[0][0]',\n",
            "                                )                                 'batch_normalization_109[0][0]']\n",
            "                                                                                                  \n",
            " activation_82 (Activation)     (None, 256, 256, 51  0           ['add_18[0][0]']                 \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_110 (Batch  (None, 256, 256, 51  204        ['activation_82[0][0]']          \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_74 (Conv2D)             (None, 256, 256, 1)  51          ['batch_normalization_110[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_111 (Batch  (None, 256, 256, 1)  3          ['conv2d_74[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_83 (Activation)     (None, 256, 256, 1)  0           ['batch_normalization_111[0][0]']\n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 10,069,640\n",
            "Trainable params: 10,040,236\n",
            "Non-trainable params: 29,404\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Defining Losses"
      ],
      "metadata": {
        "id": "HbZIWXKKD9IW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K \n",
        "'''\n",
        "# different loss functions\n",
        "def dice_coef(y_true, y_pred):\n",
        "    smooth = 1.0  #0.0\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
        "\n",
        "def jacard(y_true, y_pred):\n",
        "\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum ( y_true_f * y_pred_f)\n",
        "    union = K.sum ( y_true_f + y_pred_f - y_true_f * y_pred_f)\n",
        "\n",
        "    return intersection/union\n",
        "\n",
        "def dice_coef_loss(y_true,y_pred):\n",
        "    return 1 - dice_coef(y_true,y_pred)\n",
        "\n",
        "def iou_loss(y_true,y_pred):\n",
        "    return 1 - jacard(y_true, y_pred)\n",
        "    \n",
        "\n",
        "def tversky(y_true, y_pred):\n",
        "    y_true_pos = K.flatten(y_true)\n",
        "    y_pred_pos = K.flatten(y_pred)\n",
        "    true_pos = K.sum(y_true_pos * y_pred_pos)\n",
        "    false_neg = K.sum(y_true_pos * (1-y_pred_pos))\n",
        "    false_pos = K.sum((1-y_true_pos)*y_pred_pos)\n",
        "    alpha = 0.75\n",
        "    smooth = 1\n",
        "    return (true_pos + smooth)/(true_pos + alpha*false_neg + (1-alpha)*false_pos + smooth)\n",
        "\n",
        "\n",
        "def tversky_loss(y_true, y_pred):\n",
        "    return 1 - tversky(y_true,y_pred)\n",
        "\n",
        "\n",
        "def focal_tversky(y_true,y_pred):\n",
        "    pt_1 = tversky(y_true, y_pred)\n",
        "    gamma = 0.75\n",
        "    return K.pow((1-pt_1), gamma)\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "KAOQ8uM2uRfY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "1dd2d160-7776-4879-e23a-4bb9c280baa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# different loss functions\\ndef dice_coef(y_true, y_pred):\\n    smooth = 1.0  #0.0\\n    y_true_f = K.flatten(y_true)\\n    y_pred_f = K.flatten(y_pred)\\n    intersection = K.sum(y_true_f * y_pred_f)\\n    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\\n\\ndef jacard(y_true, y_pred):\\n\\n    y_true_f = K.flatten(y_true)\\n    y_pred_f = K.flatten(y_pred)\\n    intersection = K.sum ( y_true_f * y_pred_f)\\n    union = K.sum ( y_true_f + y_pred_f - y_true_f * y_pred_f)\\n\\n    return intersection/union\\n\\ndef dice_coef_loss(y_true,y_pred):\\n    return 1 - dice_coef(y_true,y_pred)\\n\\ndef iou_loss(y_true,y_pred):\\n    return 1 - jacard(y_true, y_pred)\\n    \\n\\ndef tversky(y_true, y_pred):\\n    y_true_pos = K.flatten(y_true)\\n    y_pred_pos = K.flatten(y_pred)\\n    true_pos = K.sum(y_true_pos * y_pred_pos)\\n    false_neg = K.sum(y_true_pos * (1-y_pred_pos))\\n    false_pos = K.sum((1-y_true_pos)*y_pred_pos)\\n    alpha = 0.75\\n    smooth = 1\\n    return (true_pos + smooth)/(true_pos + alpha*false_neg + (1-alpha)*false_pos + smooth)\\n\\n\\ndef tversky_loss(y_true, y_pred):\\n    return 1 - tversky(y_true,y_pred)\\n\\n\\ndef focal_tversky(y_true,y_pred):\\n    pt_1 = tversky(y_true, y_pred)\\n    gamma = 0.75\\n    return K.pow((1-pt_1), gamma)\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dice similarity coefficient loss, brought to you by: https://github.com/nabsabraham/focal-tversky-unet\n",
        "from tensorflow import reduce_sum\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "\n",
        "def dsc(y_true, y_pred):\n",
        "    smooth = 1.\n",
        "    y_true_f = Flatten()(y_true)\n",
        "    y_pred_f = Flatten()(y_pred)\n",
        "    intersection = reduce_sum(y_true_f * y_pred_f)\n",
        "    score = (2. * intersection + smooth) / (reduce_sum(y_true_f) + reduce_sum(y_pred_f) + smooth)\n",
        "    return score\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "    loss = 1 - dsc(y_true, y_pred)\n",
        "    return loss\n",
        "\n",
        "def bce_dice_loss(y_true, y_pred):\n",
        "    loss = binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "48DBomgSETOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jacard_coefficient from: https://github.com/bnsreenu/python_for_microscopists/blob/master/207-simple_unet_model_with_jacard.py"
      ],
      "metadata": {
        "id": "PReWQAuJrQe-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K \n",
        "\n",
        "\n",
        "def jacard_coef(y_true, y_pred):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + 1.0)\n",
        "\n",
        "\n",
        "def jacard_coef_loss(y_true, y_pred):\n",
        "    return -jacard_coef(y_true, y_pred)  # -1 ultiplied as we want to minimize this value as loss function"
      ],
      "metadata": {
        "id": "c7CLO14WrPac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train DC-UNet"
      ],
      "metadata": {
        "id": "OFR0ej4i1H-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',  loss = [jacard_coef_loss], metrics = [jacard_coef])"
      ],
      "metadata": {
        "id": "wm9CvV_MtWUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epoch =100\n",
        "history_jaccard = model.fit_generator(myGene,steps_per_epoch=20,epochs=epoch,callbacks=[model_checkpoint])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfOEkl3Mu_4F",
        "outputId": "7e6f16da-ee9b-4bd0-bd5e-2b282ad78f7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 30 images belonging to 1 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 30 images belonging to 1 classes.\n",
            "Epoch 1/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.5287 - jacard_coef: 0.5287\n",
            "Epoch 1: loss improved from inf to -0.52867, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 28s 206ms/step - loss: -0.5287 - jacard_coef: 0.5287\n",
            "Epoch 2/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.5639 - jacard_coef: 0.5639\n",
            "Epoch 2: loss improved from -0.52867 to -0.56389, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 206ms/step - loss: -0.5639 - jacard_coef: 0.5639\n",
            "Epoch 3/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.5752 - jacard_coef: 0.5752\n",
            "Epoch 3: loss improved from -0.56389 to -0.57523, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 228ms/step - loss: -0.5752 - jacard_coef: 0.5752\n",
            "Epoch 4/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.5814 - jacard_coef: 0.5814\n",
            "Epoch 4: loss improved from -0.57523 to -0.58136, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 206ms/step - loss: -0.5814 - jacard_coef: 0.5814\n",
            "Epoch 5/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.5864 - jacard_coef: 0.5864\n",
            "Epoch 5: loss improved from -0.58136 to -0.58640, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 206ms/step - loss: -0.5864 - jacard_coef: 0.5864\n",
            "Epoch 6/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.5914 - jacard_coef: 0.5914\n",
            "Epoch 6: loss improved from -0.58640 to -0.59137, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 207ms/step - loss: -0.5914 - jacard_coef: 0.5914\n",
            "Epoch 7/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.5962 - jacard_coef: 0.5962\n",
            "Epoch 7: loss improved from -0.59137 to -0.59621, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 210ms/step - loss: -0.5962 - jacard_coef: 0.5962\n",
            "Epoch 8/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.6001 - jacard_coef: 0.6001\n",
            "Epoch 8: loss improved from -0.59621 to -0.60015, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 209ms/step - loss: -0.6001 - jacard_coef: 0.6001\n",
            "Epoch 9/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.6041 - jacard_coef: 0.6041\n",
            "Epoch 9: loss improved from -0.60015 to -0.60414, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 209ms/step - loss: -0.6041 - jacard_coef: 0.6041\n",
            "Epoch 10/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.6078 - jacard_coef: 0.6078\n",
            "Epoch 10: loss improved from -0.60414 to -0.60785, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 208ms/step - loss: -0.6078 - jacard_coef: 0.6078\n",
            "Epoch 11/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.6115 - jacard_coef: 0.6115\n",
            "Epoch 11: loss improved from -0.60785 to -0.61154, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 209ms/step - loss: -0.6115 - jacard_coef: 0.6115\n",
            "Epoch 12/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.6154 - jacard_coef: 0.6154\n",
            "Epoch 12: loss improved from -0.61154 to -0.61536, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 210ms/step - loss: -0.6154 - jacard_coef: 0.6154\n",
            "Epoch 13/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.6194 - jacard_coef: 0.6194\n",
            "Epoch 13: loss improved from -0.61536 to -0.61936, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 209ms/step - loss: -0.6194 - jacard_coef: 0.6194\n",
            "Epoch 14/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.6233 - jacard_coef: 0.6233\n",
            "Epoch 14: loss improved from -0.61936 to -0.62332, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 209ms/step - loss: -0.6233 - jacard_coef: 0.6233\n",
            "Epoch 15/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.6276 - jacard_coef: 0.6276\n",
            "Epoch 15: loss improved from -0.62332 to -0.62756, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 211ms/step - loss: -0.6276 - jacard_coef: 0.6276\n",
            "Epoch 16/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.6300 - jacard_coef: 0.6300\n",
            "Epoch 16: loss improved from -0.62756 to -0.63004, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 211ms/step - loss: -0.6300 - jacard_coef: 0.6300\n",
            "Epoch 17/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.6335 - jacard_coef: 0.6335\n",
            "Epoch 17: loss improved from -0.63004 to -0.63352, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 211ms/step - loss: -0.6335 - jacard_coef: 0.6335\n",
            "Epoch 18/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.6355 - jacard_coef: 0.6355\n",
            "Epoch 18: loss improved from -0.63352 to -0.63550, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 211ms/step - loss: -0.6355 - jacard_coef: 0.6355\n",
            "Epoch 19/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.6380 - jacard_coef: 0.6380\n",
            "Epoch 19: loss improved from -0.63550 to -0.63796, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 212ms/step - loss: -0.6380 - jacard_coef: 0.6380\n",
            "Epoch 20/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.6431 - jacard_coef: 0.6431\n",
            "Epoch 20: loss improved from -0.63796 to -0.64311, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 211ms/step - loss: -0.6431 - jacard_coef: 0.6431\n",
            "Epoch 21/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.6457 - jacard_coef: 0.6457\n",
            "Epoch 21: loss improved from -0.64311 to -0.64567, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 212ms/step - loss: -0.6457 - jacard_coef: 0.6457\n",
            "Epoch 22/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.6490 - jacard_coef: 0.6490\n",
            "Epoch 22: loss improved from -0.64567 to -0.64900, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 5s 233ms/step - loss: -0.6490 - jacard_coef: 0.6490\n",
            "Epoch 23/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.6536 - jacard_coef: 0.6536\n",
            "Epoch 23: loss improved from -0.64900 to -0.65357, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 212ms/step - loss: -0.6536 - jacard_coef: 0.6536\n",
            "Epoch 24/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.6551 - jacard_coef: 0.6551\n",
            "Epoch 24: loss improved from -0.65357 to -0.65514, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 213ms/step - loss: -0.6551 - jacard_coef: 0.6551\n",
            "Epoch 25/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.6588 - jacard_coef: 0.6588\n",
            "Epoch 25: loss improved from -0.65514 to -0.65877, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 214ms/step - loss: -0.6588 - jacard_coef: 0.6588\n",
            "Epoch 26/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.6623 - jacard_coef: 0.6623\n",
            "Epoch 26: loss improved from -0.65877 to -0.66231, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 213ms/step - loss: -0.6623 - jacard_coef: 0.6623\n",
            "Epoch 27/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.6644 - jacard_coef: 0.6644\n",
            "Epoch 27: loss improved from -0.66231 to -0.66441, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 213ms/step - loss: -0.6644 - jacard_coef: 0.6644\n",
            "Epoch 28/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.6681 - jacard_coef: 0.6681\n",
            "Epoch 28: loss improved from -0.66441 to -0.66813, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 214ms/step - loss: -0.6681 - jacard_coef: 0.6681\n",
            "Epoch 29/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.6701 - jacard_coef: 0.6701\n",
            "Epoch 29: loss improved from -0.66813 to -0.67010, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 215ms/step - loss: -0.6701 - jacard_coef: 0.6701\n",
            "Epoch 30/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.6730 - jacard_coef: 0.6730\n",
            "Epoch 30: loss improved from -0.67010 to -0.67297, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 215ms/step - loss: -0.6730 - jacard_coef: 0.6730\n",
            "Epoch 31/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.6758 - jacard_coef: 0.6758\n",
            "Epoch 31: loss improved from -0.67297 to -0.67576, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 215ms/step - loss: -0.6758 - jacard_coef: 0.6758\n",
            "Epoch 32/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.6779 - jacard_coef: 0.6779\n",
            "Epoch 32: loss improved from -0.67576 to -0.67787, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 225ms/step - loss: -0.6779 - jacard_coef: 0.6779\n",
            "Epoch 33/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.6811 - jacard_coef: 0.6811\n",
            "Epoch 33: loss improved from -0.67787 to -0.68105, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 224ms/step - loss: -0.6811 - jacard_coef: 0.6811\n",
            "Epoch 34/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.6841 - jacard_coef: 0.6841\n",
            "Epoch 34: loss improved from -0.68105 to -0.68406, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 218ms/step - loss: -0.6841 - jacard_coef: 0.6841\n",
            "Epoch 35/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.6856 - jacard_coef: 0.6856\n",
            "Epoch 35: loss improved from -0.68406 to -0.68563, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 218ms/step - loss: -0.6856 - jacard_coef: 0.6856\n",
            "Epoch 36/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.6887 - jacard_coef: 0.6887\n",
            "Epoch 36: loss improved from -0.68563 to -0.68865, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 218ms/step - loss: -0.6887 - jacard_coef: 0.6887\n",
            "Epoch 37/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.6906 - jacard_coef: 0.6906\n",
            "Epoch 37: loss improved from -0.68865 to -0.69055, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 218ms/step - loss: -0.6906 - jacard_coef: 0.6906\n",
            "Epoch 38/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.6934 - jacard_coef: 0.6934\n",
            "Epoch 38: loss improved from -0.69055 to -0.69343, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 218ms/step - loss: -0.6934 - jacard_coef: 0.6934\n",
            "Epoch 39/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.6957 - jacard_coef: 0.6957\n",
            "Epoch 39: loss improved from -0.69343 to -0.69566, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 218ms/step - loss: -0.6957 - jacard_coef: 0.6957\n",
            "Epoch 40/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.6975 - jacard_coef: 0.6975\n",
            "Epoch 40: loss improved from -0.69566 to -0.69755, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 5s 239ms/step - loss: -0.6975 - jacard_coef: 0.6975\n",
            "Epoch 41/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.6991 - jacard_coef: 0.6991\n",
            "Epoch 41: loss improved from -0.69755 to -0.69906, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 219ms/step - loss: -0.6991 - jacard_coef: 0.6991\n",
            "Epoch 42/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.7011 - jacard_coef: 0.7011\n",
            "Epoch 42: loss improved from -0.69906 to -0.70113, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 219ms/step - loss: -0.7011 - jacard_coef: 0.7011\n",
            "Epoch 43/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.7044 - jacard_coef: 0.7044\n",
            "Epoch 43: loss improved from -0.70113 to -0.70436, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 218ms/step - loss: -0.7044 - jacard_coef: 0.7044\n",
            "Epoch 44/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.7054 - jacard_coef: 0.7054\n",
            "Epoch 44: loss improved from -0.70436 to -0.70542, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 218ms/step - loss: -0.7054 - jacard_coef: 0.7054\n",
            "Epoch 45/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.7088 - jacard_coef: 0.7088\n",
            "Epoch 45: loss improved from -0.70542 to -0.70880, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 218ms/step - loss: -0.7088 - jacard_coef: 0.7088\n",
            "Epoch 46/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.7092 - jacard_coef: 0.7092\n",
            "Epoch 46: loss improved from -0.70880 to -0.70925, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 218ms/step - loss: -0.7092 - jacard_coef: 0.7092\n",
            "Epoch 47/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.7138 - jacard_coef: 0.7138\n",
            "Epoch 47: loss improved from -0.70925 to -0.71376, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 219ms/step - loss: -0.7138 - jacard_coef: 0.7138\n",
            "Epoch 48/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.7142 - jacard_coef: 0.7142\n",
            "Epoch 48: loss improved from -0.71376 to -0.71419, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 218ms/step - loss: -0.7142 - jacard_coef: 0.7142\n",
            "Epoch 49/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.7157 - jacard_coef: 0.7157\n",
            "Epoch 49: loss improved from -0.71419 to -0.71572, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 219ms/step - loss: -0.7157 - jacard_coef: 0.7157\n",
            "Epoch 50/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.7164 - jacard_coef: 0.7164\n",
            "Epoch 50: loss improved from -0.71572 to -0.71642, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 217ms/step - loss: -0.7164 - jacard_coef: 0.7164\n",
            "Epoch 51/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.7212 - jacard_coef: 0.7212\n",
            "Epoch 51: loss improved from -0.71642 to -0.72121, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 218ms/step - loss: -0.7212 - jacard_coef: 0.7212\n",
            "Epoch 52/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.7208 - jacard_coef: 0.7208\n",
            "Epoch 52: loss did not improve from -0.72121\n",
            "20/20 [==============================] - 3s 167ms/step - loss: -0.7208 - jacard_coef: 0.7208\n",
            "Epoch 53/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.7241 - jacard_coef: 0.7241\n",
            "Epoch 53: loss improved from -0.72121 to -0.72409, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 219ms/step - loss: -0.7241 - jacard_coef: 0.7241\n",
            "Epoch 54/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.7247 - jacard_coef: 0.7247\n",
            "Epoch 54: loss improved from -0.72409 to -0.72469, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 217ms/step - loss: -0.7247 - jacard_coef: 0.7247\n",
            "Epoch 55/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.7265 - jacard_coef: 0.7265\n",
            "Epoch 55: loss improved from -0.72469 to -0.72649, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 218ms/step - loss: -0.7265 - jacard_coef: 0.7265\n",
            "Epoch 56/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.7283 - jacard_coef: 0.7283\n",
            "Epoch 56: loss improved from -0.72649 to -0.72835, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 5s 231ms/step - loss: -0.7283 - jacard_coef: 0.7283\n",
            "Epoch 57/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.7289 - jacard_coef: 0.7289\n",
            "Epoch 57: loss improved from -0.72835 to -0.72893, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 217ms/step - loss: -0.7289 - jacard_coef: 0.7289\n",
            "Epoch 58/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.7314 - jacard_coef: 0.7314\n",
            "Epoch 58: loss improved from -0.72893 to -0.73139, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 217ms/step - loss: -0.7314 - jacard_coef: 0.7314\n",
            "Epoch 59/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.7318 - jacard_coef: 0.7318\n",
            "Epoch 59: loss improved from -0.73139 to -0.73175, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 216ms/step - loss: -0.7318 - jacard_coef: 0.7318\n",
            "Epoch 60/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.7341 - jacard_coef: 0.7341\n",
            "Epoch 60: loss improved from -0.73175 to -0.73411, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 216ms/step - loss: -0.7341 - jacard_coef: 0.7341\n",
            "Epoch 61/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.7357 - jacard_coef: 0.7357\n",
            "Epoch 61: loss improved from -0.73411 to -0.73571, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 217ms/step - loss: -0.7357 - jacard_coef: 0.7357\n",
            "Epoch 62/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.7366 - jacard_coef: 0.7366\n",
            "Epoch 62: loss improved from -0.73571 to -0.73660, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 216ms/step - loss: -0.7366 - jacard_coef: 0.7366\n",
            "Epoch 63/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.7399 - jacard_coef: 0.7399\n",
            "Epoch 63: loss improved from -0.73660 to -0.73994, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 225ms/step - loss: -0.7399 - jacard_coef: 0.7399\n",
            "Epoch 64/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.7409 - jacard_coef: 0.7409\n",
            "Epoch 64: loss improved from -0.73994 to -0.74094, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 217ms/step - loss: -0.7409 - jacard_coef: 0.7409\n",
            "Epoch 65/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.7401 - jacard_coef: 0.7401\n",
            "Epoch 65: loss did not improve from -0.74094\n",
            "20/20 [==============================] - 3s 167ms/step - loss: -0.7401 - jacard_coef: 0.7401\n",
            "Epoch 66/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.7436 - jacard_coef: 0.7436\n",
            "Epoch 66: loss improved from -0.74094 to -0.74357, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 217ms/step - loss: -0.7436 - jacard_coef: 0.7436\n",
            "Epoch 67/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.7453 - jacard_coef: 0.7453\n",
            "Epoch 67: loss improved from -0.74357 to -0.74532, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 217ms/step - loss: -0.7453 - jacard_coef: 0.7453\n",
            "Epoch 68/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.7433 - jacard_coef: 0.7433\n",
            "Epoch 68: loss did not improve from -0.74532\n",
            "20/20 [==============================] - 3s 167ms/step - loss: -0.7433 - jacard_coef: 0.7433\n",
            "Epoch 69/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.7476 - jacard_coef: 0.7476\n",
            "Epoch 69: loss improved from -0.74532 to -0.74758, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 217ms/step - loss: -0.7476 - jacard_coef: 0.7476\n",
            "Epoch 70/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.7472 - jacard_coef: 0.7472\n",
            "Epoch 70: loss did not improve from -0.74758\n",
            "20/20 [==============================] - 3s 167ms/step - loss: -0.7472 - jacard_coef: 0.7472\n",
            "Epoch 71/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.7496 - jacard_coef: 0.7496\n",
            "Epoch 71: loss improved from -0.74758 to -0.74963, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 219ms/step - loss: -0.7496 - jacard_coef: 0.7496\n",
            "Epoch 72/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.7511 - jacard_coef: 0.7511\n",
            "Epoch 72: loss improved from -0.74963 to -0.75106, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 217ms/step - loss: -0.7511 - jacard_coef: 0.7511\n",
            "Epoch 73/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.7506 - jacard_coef: 0.7506\n",
            "Epoch 73: loss did not improve from -0.75106\n",
            "20/20 [==============================] - 3s 166ms/step - loss: -0.7506 - jacard_coef: 0.7506\n",
            "Epoch 74/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.7537 - jacard_coef: 0.7537\n",
            "Epoch 74: loss improved from -0.75106 to -0.75366, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 216ms/step - loss: -0.7537 - jacard_coef: 0.7537\n",
            "Epoch 75/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.7538 - jacard_coef: 0.7538\n",
            "Epoch 75: loss improved from -0.75366 to -0.75381, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 216ms/step - loss: -0.7538 - jacard_coef: 0.7538\n",
            "Epoch 76/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.7545 - jacard_coef: 0.7545\n",
            "Epoch 76: loss improved from -0.75381 to -0.75453, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 216ms/step - loss: -0.7545 - jacard_coef: 0.7545\n",
            "Epoch 77/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.7556 - jacard_coef: 0.7556\n",
            "Epoch 77: loss improved from -0.75453 to -0.75560, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 216ms/step - loss: -0.7556 - jacard_coef: 0.7556\n",
            "Epoch 78/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.7574 - jacard_coef: 0.7574\n",
            "Epoch 78: loss improved from -0.75560 to -0.75742, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 216ms/step - loss: -0.7574 - jacard_coef: 0.7574\n",
            "Epoch 79/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.7584 - jacard_coef: 0.7584\n",
            "Epoch 79: loss improved from -0.75742 to -0.75840, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 216ms/step - loss: -0.7584 - jacard_coef: 0.7584\n",
            "Epoch 80/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.7596 - jacard_coef: 0.7596\n",
            "Epoch 80: loss improved from -0.75840 to -0.75963, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 216ms/step - loss: -0.7596 - jacard_coef: 0.7596\n",
            "Epoch 81/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.7585 - jacard_coef: 0.7585\n",
            "Epoch 81: loss did not improve from -0.75963\n",
            "20/20 [==============================] - 3s 167ms/step - loss: -0.7585 - jacard_coef: 0.7585\n",
            "Epoch 82/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.7599 - jacard_coef: 0.7599\n",
            "Epoch 82: loss improved from -0.75963 to -0.75994, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 218ms/step - loss: -0.7599 - jacard_coef: 0.7599\n",
            "Epoch 83/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.7623 - jacard_coef: 0.7623\n",
            "Epoch 83: loss improved from -0.75994 to -0.76228, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 217ms/step - loss: -0.7623 - jacard_coef: 0.7623\n",
            "Epoch 84/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.7628 - jacard_coef: 0.7628\n",
            "Epoch 84: loss improved from -0.76228 to -0.76284, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 216ms/step - loss: -0.7628 - jacard_coef: 0.7628\n",
            "Epoch 85/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.7646 - jacard_coef: 0.7646\n",
            "Epoch 85: loss improved from -0.76284 to -0.76463, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 217ms/step - loss: -0.7646 - jacard_coef: 0.7646\n",
            "Epoch 86/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.7635 - jacard_coef: 0.7635\n",
            "Epoch 86: loss did not improve from -0.76463\n",
            "20/20 [==============================] - 3s 168ms/step - loss: -0.7635 - jacard_coef: 0.7635\n",
            "Epoch 87/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.7647 - jacard_coef: 0.7647\n",
            "Epoch 87: loss improved from -0.76463 to -0.76473, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 217ms/step - loss: -0.7647 - jacard_coef: 0.7647\n",
            "Epoch 88/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.7664 - jacard_coef: 0.7664\n",
            "Epoch 88: loss improved from -0.76473 to -0.76645, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 217ms/step - loss: -0.7664 - jacard_coef: 0.7664\n",
            "Epoch 89/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.7649 - jacard_coef: 0.7649\n",
            "Epoch 89: loss did not improve from -0.76645\n",
            "20/20 [==============================] - 3s 168ms/step - loss: -0.7649 - jacard_coef: 0.7649\n",
            "Epoch 90/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.7697 - jacard_coef: 0.7697\n",
            "Epoch 90: loss improved from -0.76645 to -0.76967, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 217ms/step - loss: -0.7697 - jacard_coef: 0.7697\n",
            "Epoch 91/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.7667 - jacard_coef: 0.7667\n",
            "Epoch 91: loss did not improve from -0.76967\n",
            "20/20 [==============================] - 3s 168ms/step - loss: -0.7667 - jacard_coef: 0.7667\n",
            "Epoch 92/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.7691 - jacard_coef: 0.7691\n",
            "Epoch 92: loss did not improve from -0.76967\n",
            "20/20 [==============================] - 3s 168ms/step - loss: -0.7691 - jacard_coef: 0.7691\n",
            "Epoch 93/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.7713 - jacard_coef: 0.7713\n",
            "Epoch 93: loss improved from -0.76967 to -0.77130, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 217ms/step - loss: -0.7713 - jacard_coef: 0.7713\n",
            "Epoch 94/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.7701 - jacard_coef: 0.7701\n",
            "Epoch 94: loss did not improve from -0.77130\n",
            "20/20 [==============================] - 3s 169ms/step - loss: -0.7701 - jacard_coef: 0.7701\n",
            "Epoch 95/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.7712 - jacard_coef: 0.7712\n",
            "Epoch 95: loss did not improve from -0.77130\n",
            "20/20 [==============================] - 3s 167ms/step - loss: -0.7712 - jacard_coef: 0.7712\n",
            "Epoch 96/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.7723 - jacard_coef: 0.7723\n",
            "Epoch 96: loss improved from -0.77130 to -0.77230, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 218ms/step - loss: -0.7723 - jacard_coef: 0.7723\n",
            "Epoch 97/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.7724 - jacard_coef: 0.7724\n",
            "Epoch 97: loss improved from -0.77230 to -0.77241, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 218ms/step - loss: -0.7724 - jacard_coef: 0.7724\n",
            "Epoch 98/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.7750 - jacard_coef: 0.7750\n",
            "Epoch 98: loss improved from -0.77241 to -0.77503, saving model to DCUNet_membrane.hdf5\n",
            "20/20 [==============================] - 4s 216ms/step - loss: -0.7750 - jacard_coef: 0.7750\n",
            "Epoch 99/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.7735 - jacard_coef: 0.7735\n",
            "Epoch 99: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 167ms/step - loss: -0.7735 - jacard_coef: 0.7735\n",
            "Epoch 100/100\n",
            "20/20 [==============================] - ETA: 0s - loss: -0.7750 - jacard_coef: 0.7750\n",
            "Epoch 100: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 167ms/step - loss: -0.7750 - jacard_coef: 0.7750\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=bce_dice_loss, metrics=[dice_loss,'accuracy'])"
      ],
      "metadata": {
        "id": "z_9jvERE05Cf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_dice = model.fit(myGene,steps_per_epoch=20,epochs=epoch,callbacks=[model_checkpoint])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIcHIv6m05gv",
        "outputId": "61048b79-de00-43b0-e8cb-949158f7a45f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3887 - dice_loss: 0.1282 - accuracy: 0.9343\n",
            "Epoch 1: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 11s 168ms/step - loss: 0.3887 - dice_loss: 0.1282 - accuracy: 0.9343\n",
            "Epoch 2/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3883 - dice_loss: 0.1278 - accuracy: 0.9314\n",
            "Epoch 2: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 167ms/step - loss: 0.3883 - dice_loss: 0.1278 - accuracy: 0.9314\n",
            "Epoch 3/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3850 - dice_loss: 0.1266 - accuracy: 0.9321\n",
            "Epoch 3: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 168ms/step - loss: 0.3850 - dice_loss: 0.1266 - accuracy: 0.9321\n",
            "Epoch 4/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3862 - dice_loss: 0.1268 - accuracy: 0.9311\n",
            "Epoch 4: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3862 - dice_loss: 0.1268 - accuracy: 0.9311\n",
            "Epoch 5/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3854 - dice_loss: 0.1259 - accuracy: 0.9294\n",
            "Epoch 5: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 170ms/step - loss: 0.3854 - dice_loss: 0.1259 - accuracy: 0.9294\n",
            "Epoch 6/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3884 - dice_loss: 0.1268 - accuracy: 0.9279\n",
            "Epoch 6: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 171ms/step - loss: 0.3884 - dice_loss: 0.1268 - accuracy: 0.9279\n",
            "Epoch 7/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3801 - dice_loss: 0.1242 - accuracy: 0.9311\n",
            "Epoch 7: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 171ms/step - loss: 0.3801 - dice_loss: 0.1242 - accuracy: 0.9311\n",
            "Epoch 8/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3858 - dice_loss: 0.1255 - accuracy: 0.9273\n",
            "Epoch 8: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 172ms/step - loss: 0.3858 - dice_loss: 0.1255 - accuracy: 0.9273\n",
            "Epoch 9/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3815 - dice_loss: 0.1243 - accuracy: 0.9297\n",
            "Epoch 9: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 172ms/step - loss: 0.3815 - dice_loss: 0.1243 - accuracy: 0.9297\n",
            "Epoch 10/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3821 - dice_loss: 0.1243 - accuracy: 0.9283\n",
            "Epoch 10: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 173ms/step - loss: 0.3821 - dice_loss: 0.1243 - accuracy: 0.9283\n",
            "Epoch 11/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3837 - dice_loss: 0.1247 - accuracy: 0.9279\n",
            "Epoch 11: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 171ms/step - loss: 0.3837 - dice_loss: 0.1247 - accuracy: 0.9279\n",
            "Epoch 12/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3819 - dice_loss: 0.1239 - accuracy: 0.9278\n",
            "Epoch 12: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 172ms/step - loss: 0.3819 - dice_loss: 0.1239 - accuracy: 0.9278\n",
            "Epoch 13/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3804 - dice_loss: 0.1235 - accuracy: 0.9290\n",
            "Epoch 13: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 172ms/step - loss: 0.3804 - dice_loss: 0.1235 - accuracy: 0.9290\n",
            "Epoch 14/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3799 - dice_loss: 0.1232 - accuracy: 0.9278\n",
            "Epoch 14: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 170ms/step - loss: 0.3799 - dice_loss: 0.1232 - accuracy: 0.9278\n",
            "Epoch 15/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3789 - dice_loss: 0.1229 - accuracy: 0.9292\n",
            "Epoch 15: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3789 - dice_loss: 0.1229 - accuracy: 0.9292\n",
            "Epoch 16/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3843 - dice_loss: 0.1241 - accuracy: 0.9251\n",
            "Epoch 16: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3843 - dice_loss: 0.1241 - accuracy: 0.9251\n",
            "Epoch 17/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3820 - dice_loss: 0.1236 - accuracy: 0.9261\n",
            "Epoch 17: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 168ms/step - loss: 0.3820 - dice_loss: 0.1236 - accuracy: 0.9261\n",
            "Epoch 18/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3799 - dice_loss: 0.1228 - accuracy: 0.9268\n",
            "Epoch 18: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 168ms/step - loss: 0.3799 - dice_loss: 0.1228 - accuracy: 0.9268\n",
            "Epoch 19/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3846 - dice_loss: 0.1243 - accuracy: 0.9246\n",
            "Epoch 19: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 168ms/step - loss: 0.3846 - dice_loss: 0.1243 - accuracy: 0.9246\n",
            "Epoch 20/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3760 - dice_loss: 0.1218 - accuracy: 0.9293\n",
            "Epoch 20: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 168ms/step - loss: 0.3760 - dice_loss: 0.1218 - accuracy: 0.9293\n",
            "Epoch 21/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3760 - dice_loss: 0.1218 - accuracy: 0.9291\n",
            "Epoch 21: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 168ms/step - loss: 0.3760 - dice_loss: 0.1218 - accuracy: 0.9291\n",
            "Epoch 22/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3854 - dice_loss: 0.1245 - accuracy: 0.9250\n",
            "Epoch 22: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 168ms/step - loss: 0.3854 - dice_loss: 0.1245 - accuracy: 0.9250\n",
            "Epoch 23/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3774 - dice_loss: 0.1222 - accuracy: 0.9290\n",
            "Epoch 23: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 168ms/step - loss: 0.3774 - dice_loss: 0.1222 - accuracy: 0.9290\n",
            "Epoch 24/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3757 - dice_loss: 0.1217 - accuracy: 0.9293\n",
            "Epoch 24: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 167ms/step - loss: 0.3757 - dice_loss: 0.1217 - accuracy: 0.9293\n",
            "Epoch 25/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3761 - dice_loss: 0.1217 - accuracy: 0.9287\n",
            "Epoch 25: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 167ms/step - loss: 0.3761 - dice_loss: 0.1217 - accuracy: 0.9287\n",
            "Epoch 26/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3800 - dice_loss: 0.1228 - accuracy: 0.9268\n",
            "Epoch 26: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 167ms/step - loss: 0.3800 - dice_loss: 0.1228 - accuracy: 0.9268\n",
            "Epoch 27/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3751 - dice_loss: 0.1214 - accuracy: 0.9302\n",
            "Epoch 27: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 168ms/step - loss: 0.3751 - dice_loss: 0.1214 - accuracy: 0.9302\n",
            "Epoch 28/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3755 - dice_loss: 0.1214 - accuracy: 0.9288\n",
            "Epoch 28: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 168ms/step - loss: 0.3755 - dice_loss: 0.1214 - accuracy: 0.9288\n",
            "Epoch 29/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3780 - dice_loss: 0.1223 - accuracy: 0.9289\n",
            "Epoch 29: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 168ms/step - loss: 0.3780 - dice_loss: 0.1223 - accuracy: 0.9289\n",
            "Epoch 30/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3786 - dice_loss: 0.1224 - accuracy: 0.9278\n",
            "Epoch 30: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 168ms/step - loss: 0.3786 - dice_loss: 0.1224 - accuracy: 0.9278\n",
            "Epoch 31/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3849 - dice_loss: 0.1243 - accuracy: 0.9253\n",
            "Epoch 31: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3849 - dice_loss: 0.1243 - accuracy: 0.9253\n",
            "Epoch 32/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3737 - dice_loss: 0.1210 - accuracy: 0.9297\n",
            "Epoch 32: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3737 - dice_loss: 0.1210 - accuracy: 0.9297\n",
            "Epoch 33/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3721 - dice_loss: 0.1206 - accuracy: 0.9303\n",
            "Epoch 33: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3721 - dice_loss: 0.1206 - accuracy: 0.9303\n",
            "Epoch 34/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3839 - dice_loss: 0.1240 - accuracy: 0.9254\n",
            "Epoch 34: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 170ms/step - loss: 0.3839 - dice_loss: 0.1240 - accuracy: 0.9254\n",
            "Epoch 35/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3739 - dice_loss: 0.1214 - accuracy: 0.9310\n",
            "Epoch 35: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 170ms/step - loss: 0.3739 - dice_loss: 0.1214 - accuracy: 0.9310\n",
            "Epoch 36/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3726 - dice_loss: 0.1209 - accuracy: 0.9319\n",
            "Epoch 36: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 170ms/step - loss: 0.3726 - dice_loss: 0.1209 - accuracy: 0.9319\n",
            "Epoch 37/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3755 - dice_loss: 0.1217 - accuracy: 0.9305\n",
            "Epoch 37: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3755 - dice_loss: 0.1217 - accuracy: 0.9305\n",
            "Epoch 38/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3775 - dice_loss: 0.1222 - accuracy: 0.9286\n",
            "Epoch 38: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 170ms/step - loss: 0.3775 - dice_loss: 0.1222 - accuracy: 0.9286\n",
            "Epoch 39/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3753 - dice_loss: 0.1216 - accuracy: 0.9306\n",
            "Epoch 39: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 170ms/step - loss: 0.3753 - dice_loss: 0.1216 - accuracy: 0.9306\n",
            "Epoch 40/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3720 - dice_loss: 0.1208 - accuracy: 0.9320\n",
            "Epoch 40: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3720 - dice_loss: 0.1208 - accuracy: 0.9320\n",
            "Epoch 41/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3817 - dice_loss: 0.1235 - accuracy: 0.9275\n",
            "Epoch 41: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 168ms/step - loss: 0.3817 - dice_loss: 0.1235 - accuracy: 0.9275\n",
            "Epoch 42/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3781 - dice_loss: 0.1225 - accuracy: 0.9293\n",
            "Epoch 42: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3781 - dice_loss: 0.1225 - accuracy: 0.9293\n",
            "Epoch 43/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3761 - dice_loss: 0.1221 - accuracy: 0.9306\n",
            "Epoch 43: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3761 - dice_loss: 0.1221 - accuracy: 0.9306\n",
            "Epoch 44/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3746 - dice_loss: 0.1216 - accuracy: 0.9309\n",
            "Epoch 44: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 168ms/step - loss: 0.3746 - dice_loss: 0.1216 - accuracy: 0.9309\n",
            "Epoch 45/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3732 - dice_loss: 0.1212 - accuracy: 0.9320\n",
            "Epoch 45: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 168ms/step - loss: 0.3732 - dice_loss: 0.1212 - accuracy: 0.9320\n",
            "Epoch 46/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3723 - dice_loss: 0.1209 - accuracy: 0.9319\n",
            "Epoch 46: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 168ms/step - loss: 0.3723 - dice_loss: 0.1209 - accuracy: 0.9319\n",
            "Epoch 47/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3743 - dice_loss: 0.1216 - accuracy: 0.9315\n",
            "Epoch 47: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 168ms/step - loss: 0.3743 - dice_loss: 0.1216 - accuracy: 0.9315\n",
            "Epoch 48/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3678 - dice_loss: 0.1197 - accuracy: 0.9346\n",
            "Epoch 48: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3678 - dice_loss: 0.1197 - accuracy: 0.9346\n",
            "Epoch 49/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3751 - dice_loss: 0.1218 - accuracy: 0.9314\n",
            "Epoch 49: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3751 - dice_loss: 0.1218 - accuracy: 0.9314\n",
            "Epoch 50/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3803 - dice_loss: 0.1232 - accuracy: 0.9285\n",
            "Epoch 50: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3803 - dice_loss: 0.1232 - accuracy: 0.9285\n",
            "Epoch 51/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3703 - dice_loss: 0.1206 - accuracy: 0.9337\n",
            "Epoch 51: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3703 - dice_loss: 0.1206 - accuracy: 0.9337\n",
            "Epoch 52/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3734 - dice_loss: 0.1214 - accuracy: 0.9329\n",
            "Epoch 52: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 168ms/step - loss: 0.3734 - dice_loss: 0.1214 - accuracy: 0.9329\n",
            "Epoch 53/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3721 - dice_loss: 0.1210 - accuracy: 0.9331\n",
            "Epoch 53: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3721 - dice_loss: 0.1210 - accuracy: 0.9331\n",
            "Epoch 54/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3745 - dice_loss: 0.1218 - accuracy: 0.9324\n",
            "Epoch 54: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 170ms/step - loss: 0.3745 - dice_loss: 0.1218 - accuracy: 0.9324\n",
            "Epoch 55/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3669 - dice_loss: 0.1195 - accuracy: 0.9358\n",
            "Epoch 55: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3669 - dice_loss: 0.1195 - accuracy: 0.9358\n",
            "Epoch 56/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3730 - dice_loss: 0.1212 - accuracy: 0.9326\n",
            "Epoch 56: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3730 - dice_loss: 0.1212 - accuracy: 0.9326\n",
            "Epoch 57/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3712 - dice_loss: 0.1208 - accuracy: 0.9340\n",
            "Epoch 57: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3712 - dice_loss: 0.1208 - accuracy: 0.9340\n",
            "Epoch 58/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3679 - dice_loss: 0.1198 - accuracy: 0.9356\n",
            "Epoch 58: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3679 - dice_loss: 0.1198 - accuracy: 0.9356\n",
            "Epoch 59/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3709 - dice_loss: 0.1207 - accuracy: 0.9344\n",
            "Epoch 59: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 170ms/step - loss: 0.3709 - dice_loss: 0.1207 - accuracy: 0.9344\n",
            "Epoch 60/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3713 - dice_loss: 0.1208 - accuracy: 0.9336\n",
            "Epoch 60: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 170ms/step - loss: 0.3713 - dice_loss: 0.1208 - accuracy: 0.9336\n",
            "Epoch 61/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3713 - dice_loss: 0.1208 - accuracy: 0.9341\n",
            "Epoch 61: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3713 - dice_loss: 0.1208 - accuracy: 0.9341\n",
            "Epoch 62/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3688 - dice_loss: 0.1201 - accuracy: 0.9351\n",
            "Epoch 62: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3688 - dice_loss: 0.1201 - accuracy: 0.9351\n",
            "Epoch 63/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3659 - dice_loss: 0.1193 - accuracy: 0.9370\n",
            "Epoch 63: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3659 - dice_loss: 0.1193 - accuracy: 0.9370\n",
            "Epoch 64/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3673 - dice_loss: 0.1195 - accuracy: 0.9353\n",
            "Epoch 64: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 171ms/step - loss: 0.3673 - dice_loss: 0.1195 - accuracy: 0.9353\n",
            "Epoch 65/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3761 - dice_loss: 0.1221 - accuracy: 0.9317\n",
            "Epoch 65: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 171ms/step - loss: 0.3761 - dice_loss: 0.1221 - accuracy: 0.9317\n",
            "Epoch 66/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3732 - dice_loss: 0.1214 - accuracy: 0.9331\n",
            "Epoch 66: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 170ms/step - loss: 0.3732 - dice_loss: 0.1214 - accuracy: 0.9331\n",
            "Epoch 67/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3698 - dice_loss: 0.1205 - accuracy: 0.9356\n",
            "Epoch 67: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3698 - dice_loss: 0.1205 - accuracy: 0.9356\n",
            "Epoch 68/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3697 - dice_loss: 0.1202 - accuracy: 0.9347\n",
            "Epoch 68: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3697 - dice_loss: 0.1202 - accuracy: 0.9347\n",
            "Epoch 69/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3655 - dice_loss: 0.1192 - accuracy: 0.9365\n",
            "Epoch 69: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 170ms/step - loss: 0.3655 - dice_loss: 0.1192 - accuracy: 0.9365\n",
            "Epoch 70/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3792 - dice_loss: 0.1232 - accuracy: 0.9309\n",
            "Epoch 70: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3792 - dice_loss: 0.1232 - accuracy: 0.9309\n",
            "Epoch 71/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3703 - dice_loss: 0.1205 - accuracy: 0.9347\n",
            "Epoch 71: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 168ms/step - loss: 0.3703 - dice_loss: 0.1205 - accuracy: 0.9347\n",
            "Epoch 72/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3685 - dice_loss: 0.1202 - accuracy: 0.9358\n",
            "Epoch 72: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3685 - dice_loss: 0.1202 - accuracy: 0.9358\n",
            "Epoch 73/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3723 - dice_loss: 0.1213 - accuracy: 0.9342\n",
            "Epoch 73: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3723 - dice_loss: 0.1213 - accuracy: 0.9342\n",
            "Epoch 74/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3670 - dice_loss: 0.1198 - accuracy: 0.9373\n",
            "Epoch 74: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 168ms/step - loss: 0.3670 - dice_loss: 0.1198 - accuracy: 0.9373\n",
            "Epoch 75/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3713 - dice_loss: 0.1210 - accuracy: 0.9356\n",
            "Epoch 75: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3713 - dice_loss: 0.1210 - accuracy: 0.9356\n",
            "Epoch 76/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3648 - dice_loss: 0.1192 - accuracy: 0.9381\n",
            "Epoch 76: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3648 - dice_loss: 0.1192 - accuracy: 0.9381\n",
            "Epoch 77/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3746 - dice_loss: 0.1221 - accuracy: 0.9344\n",
            "Epoch 77: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3746 - dice_loss: 0.1221 - accuracy: 0.9344\n",
            "Epoch 78/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3721 - dice_loss: 0.1213 - accuracy: 0.9350\n",
            "Epoch 78: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 168ms/step - loss: 0.3721 - dice_loss: 0.1213 - accuracy: 0.9350\n",
            "Epoch 79/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3684 - dice_loss: 0.1203 - accuracy: 0.9364\n",
            "Epoch 79: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3684 - dice_loss: 0.1203 - accuracy: 0.9364\n",
            "Epoch 80/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3663 - dice_loss: 0.1198 - accuracy: 0.9383\n",
            "Epoch 80: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 170ms/step - loss: 0.3663 - dice_loss: 0.1198 - accuracy: 0.9383\n",
            "Epoch 81/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3685 - dice_loss: 0.1202 - accuracy: 0.9367\n",
            "Epoch 81: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 170ms/step - loss: 0.3685 - dice_loss: 0.1202 - accuracy: 0.9367\n",
            "Epoch 82/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3672 - dice_loss: 0.1199 - accuracy: 0.9376\n",
            "Epoch 82: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3672 - dice_loss: 0.1199 - accuracy: 0.9376\n",
            "Epoch 83/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3689 - dice_loss: 0.1204 - accuracy: 0.9369\n",
            "Epoch 83: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3689 - dice_loss: 0.1204 - accuracy: 0.9369\n",
            "Epoch 84/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3711 - dice_loss: 0.1210 - accuracy: 0.9361\n",
            "Epoch 84: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 170ms/step - loss: 0.3711 - dice_loss: 0.1210 - accuracy: 0.9361\n",
            "Epoch 85/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3654 - dice_loss: 0.1194 - accuracy: 0.9384\n",
            "Epoch 85: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3654 - dice_loss: 0.1194 - accuracy: 0.9384\n",
            "Epoch 86/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3680 - dice_loss: 0.1202 - accuracy: 0.9378\n",
            "Epoch 86: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 170ms/step - loss: 0.3680 - dice_loss: 0.1202 - accuracy: 0.9378\n",
            "Epoch 87/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3680 - dice_loss: 0.1201 - accuracy: 0.9373\n",
            "Epoch 87: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 170ms/step - loss: 0.3680 - dice_loss: 0.1201 - accuracy: 0.9373\n",
            "Epoch 88/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3659 - dice_loss: 0.1196 - accuracy: 0.9379\n",
            "Epoch 88: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3659 - dice_loss: 0.1196 - accuracy: 0.9379\n",
            "Epoch 89/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3668 - dice_loss: 0.1198 - accuracy: 0.9379\n",
            "Epoch 89: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 171ms/step - loss: 0.3668 - dice_loss: 0.1198 - accuracy: 0.9379\n",
            "Epoch 90/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3707 - dice_loss: 0.1211 - accuracy: 0.9365\n",
            "Epoch 90: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 170ms/step - loss: 0.3707 - dice_loss: 0.1211 - accuracy: 0.9365\n",
            "Epoch 91/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3660 - dice_loss: 0.1197 - accuracy: 0.9396\n",
            "Epoch 91: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3660 - dice_loss: 0.1197 - accuracy: 0.9396\n",
            "Epoch 92/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3670 - dice_loss: 0.1200 - accuracy: 0.9389\n",
            "Epoch 92: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3670 - dice_loss: 0.1200 - accuracy: 0.9389\n",
            "Epoch 93/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3687 - dice_loss: 0.1206 - accuracy: 0.9377\n",
            "Epoch 93: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 170ms/step - loss: 0.3687 - dice_loss: 0.1206 - accuracy: 0.9377\n",
            "Epoch 94/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3657 - dice_loss: 0.1196 - accuracy: 0.9393\n",
            "Epoch 94: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3657 - dice_loss: 0.1196 - accuracy: 0.9393\n",
            "Epoch 95/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3645 - dice_loss: 0.1193 - accuracy: 0.9401\n",
            "Epoch 95: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 168ms/step - loss: 0.3645 - dice_loss: 0.1193 - accuracy: 0.9401\n",
            "Epoch 96/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3641 - dice_loss: 0.1191 - accuracy: 0.9394\n",
            "Epoch 96: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3641 - dice_loss: 0.1191 - accuracy: 0.9394\n",
            "Epoch 97/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3729 - dice_loss: 0.1217 - accuracy: 0.9361\n",
            "Epoch 97: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 168ms/step - loss: 0.3729 - dice_loss: 0.1217 - accuracy: 0.9361\n",
            "Epoch 98/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3670 - dice_loss: 0.1200 - accuracy: 0.9387\n",
            "Epoch 98: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 170ms/step - loss: 0.3670 - dice_loss: 0.1200 - accuracy: 0.9387\n",
            "Epoch 99/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3699 - dice_loss: 0.1209 - accuracy: 0.9380\n",
            "Epoch 99: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 170ms/step - loss: 0.3699 - dice_loss: 0.1209 - accuracy: 0.9380\n",
            "Epoch 100/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3632 - dice_loss: 0.1189 - accuracy: 0.9403\n",
            "Epoch 100: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3632 - dice_loss: 0.1189 - accuracy: 0.9403\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "testGene = testGenerator(\"/content/drive/MyDrive/membrane - DCUNet/test\")\n",
        "results = model.predict_generator(testGene,30,verbose=1)\n",
        "saveResult(\"/content/drive/MyDrive/membrane/test\",results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N76TJtHDu_w3",
        "outputId": "4d3f1166-ddce-432f-f380-9a642d0a1261"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30/30 [==============================] - 11s 263ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit_generator(myGene,steps_per_epoch=20,epochs=epoch,callbacks=[model_checkpoint])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71V4IC1plpKK",
        "outputId": "b557013b-306f-4e56-cb6b-39564a6d460c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20/20 [==============================] - ETA: 0s - loss: 0.3625 - dice_loss: 0.1188 - accuracy: 0.9410\n",
            "Epoch 1: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3625 - dice_loss: 0.1188 - accuracy: 0.9410\n",
            "Epoch 2/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3703 - dice_loss: 0.1211 - accuracy: 0.9381\n",
            "Epoch 2: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 168ms/step - loss: 0.3703 - dice_loss: 0.1211 - accuracy: 0.9381\n",
            "Epoch 3/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3615 - dice_loss: 0.1184 - accuracy: 0.9413\n",
            "Epoch 3: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3615 - dice_loss: 0.1184 - accuracy: 0.9413\n",
            "Epoch 4/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3681 - dice_loss: 0.1204 - accuracy: 0.9379\n",
            "Epoch 4: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 170ms/step - loss: 0.3681 - dice_loss: 0.1204 - accuracy: 0.9379\n",
            "Epoch 5/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3693 - dice_loss: 0.1207 - accuracy: 0.9380\n",
            "Epoch 5: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 170ms/step - loss: 0.3693 - dice_loss: 0.1207 - accuracy: 0.9380\n",
            "Epoch 6/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3665 - dice_loss: 0.1200 - accuracy: 0.9394\n",
            "Epoch 6: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 172ms/step - loss: 0.3665 - dice_loss: 0.1200 - accuracy: 0.9394\n",
            "Epoch 7/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3660 - dice_loss: 0.1199 - accuracy: 0.9404\n",
            "Epoch 7: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 171ms/step - loss: 0.3660 - dice_loss: 0.1199 - accuracy: 0.9404\n",
            "Epoch 8/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3658 - dice_loss: 0.1198 - accuracy: 0.9406\n",
            "Epoch 8: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 172ms/step - loss: 0.3658 - dice_loss: 0.1198 - accuracy: 0.9406\n",
            "Epoch 9/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3617 - dice_loss: 0.1185 - accuracy: 0.9412\n",
            "Epoch 9: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 173ms/step - loss: 0.3617 - dice_loss: 0.1185 - accuracy: 0.9412\n",
            "Epoch 10/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3710 - dice_loss: 0.1212 - accuracy: 0.9373\n",
            "Epoch 10: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 172ms/step - loss: 0.3710 - dice_loss: 0.1212 - accuracy: 0.9373\n",
            "Epoch 11/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3627 - dice_loss: 0.1189 - accuracy: 0.9416\n",
            "Epoch 11: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 171ms/step - loss: 0.3627 - dice_loss: 0.1189 - accuracy: 0.9416\n",
            "Epoch 12/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3634 - dice_loss: 0.1192 - accuracy: 0.9421\n",
            "Epoch 12: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 170ms/step - loss: 0.3634 - dice_loss: 0.1192 - accuracy: 0.9421\n",
            "Epoch 13/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3701 - dice_loss: 0.1210 - accuracy: 0.9381\n",
            "Epoch 13: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3701 - dice_loss: 0.1210 - accuracy: 0.9381\n",
            "Epoch 14/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3615 - dice_loss: 0.1185 - accuracy: 0.9418\n",
            "Epoch 14: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 170ms/step - loss: 0.3615 - dice_loss: 0.1185 - accuracy: 0.9418\n",
            "Epoch 15/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3655 - dice_loss: 0.1197 - accuracy: 0.9400\n",
            "Epoch 15: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3655 - dice_loss: 0.1197 - accuracy: 0.9400\n",
            "Epoch 16/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3686 - dice_loss: 0.1206 - accuracy: 0.9393\n",
            "Epoch 16: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 168ms/step - loss: 0.3686 - dice_loss: 0.1206 - accuracy: 0.9393\n",
            "Epoch 17/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3652 - dice_loss: 0.1197 - accuracy: 0.9408\n",
            "Epoch 17: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 168ms/step - loss: 0.3652 - dice_loss: 0.1197 - accuracy: 0.9408\n",
            "Epoch 18/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3651 - dice_loss: 0.1198 - accuracy: 0.9412\n",
            "Epoch 18: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 168ms/step - loss: 0.3651 - dice_loss: 0.1198 - accuracy: 0.9412\n",
            "Epoch 19/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3652 - dice_loss: 0.1197 - accuracy: 0.9407\n",
            "Epoch 19: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 168ms/step - loss: 0.3652 - dice_loss: 0.1197 - accuracy: 0.9407\n",
            "Epoch 20/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3624 - dice_loss: 0.1189 - accuracy: 0.9423\n",
            "Epoch 20: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 167ms/step - loss: 0.3624 - dice_loss: 0.1189 - accuracy: 0.9423\n",
            "Epoch 21/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3655 - dice_loss: 0.1197 - accuracy: 0.9401\n",
            "Epoch 21: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 167ms/step - loss: 0.3655 - dice_loss: 0.1197 - accuracy: 0.9401\n",
            "Epoch 22/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3654 - dice_loss: 0.1198 - accuracy: 0.9411\n",
            "Epoch 22: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 167ms/step - loss: 0.3654 - dice_loss: 0.1198 - accuracy: 0.9411\n",
            "Epoch 23/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3596 - dice_loss: 0.1181 - accuracy: 0.9439\n",
            "Epoch 23: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 167ms/step - loss: 0.3596 - dice_loss: 0.1181 - accuracy: 0.9439\n",
            "Epoch 24/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3612 - dice_loss: 0.1184 - accuracy: 0.9423\n",
            "Epoch 24: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 168ms/step - loss: 0.3612 - dice_loss: 0.1184 - accuracy: 0.9423\n",
            "Epoch 25/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3665 - dice_loss: 0.1201 - accuracy: 0.9406\n",
            "Epoch 25: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 168ms/step - loss: 0.3665 - dice_loss: 0.1201 - accuracy: 0.9406\n",
            "Epoch 26/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3633 - dice_loss: 0.1191 - accuracy: 0.9420\n",
            "Epoch 26: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 167ms/step - loss: 0.3633 - dice_loss: 0.1191 - accuracy: 0.9420\n",
            "Epoch 27/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3677 - dice_loss: 0.1205 - accuracy: 0.9407\n",
            "Epoch 27: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 168ms/step - loss: 0.3677 - dice_loss: 0.1205 - accuracy: 0.9407\n",
            "Epoch 28/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3652 - dice_loss: 0.1198 - accuracy: 0.9419\n",
            "Epoch 28: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 168ms/step - loss: 0.3652 - dice_loss: 0.1198 - accuracy: 0.9419\n",
            "Epoch 29/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3631 - dice_loss: 0.1191 - accuracy: 0.9421\n",
            "Epoch 29: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 168ms/step - loss: 0.3631 - dice_loss: 0.1191 - accuracy: 0.9421\n",
            "Epoch 30/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3627 - dice_loss: 0.1191 - accuracy: 0.9426\n",
            "Epoch 30: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3627 - dice_loss: 0.1191 - accuracy: 0.9426\n",
            "Epoch 31/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3621 - dice_loss: 0.1188 - accuracy: 0.9430\n",
            "Epoch 31: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3621 - dice_loss: 0.1188 - accuracy: 0.9430\n",
            "Epoch 32/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3649 - dice_loss: 0.1197 - accuracy: 0.9418\n",
            "Epoch 32: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3649 - dice_loss: 0.1197 - accuracy: 0.9418\n",
            "Epoch 33/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3622 - dice_loss: 0.1188 - accuracy: 0.9424\n",
            "Epoch 33: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 170ms/step - loss: 0.3622 - dice_loss: 0.1188 - accuracy: 0.9424\n",
            "Epoch 34/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3661 - dice_loss: 0.1201 - accuracy: 0.9417\n",
            "Epoch 34: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3661 - dice_loss: 0.1201 - accuracy: 0.9417\n",
            "Epoch 35/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3645 - dice_loss: 0.1195 - accuracy: 0.9422\n",
            "Epoch 35: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 170ms/step - loss: 0.3645 - dice_loss: 0.1195 - accuracy: 0.9422\n",
            "Epoch 36/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3587 - dice_loss: 0.1178 - accuracy: 0.9445\n",
            "Epoch 36: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 170ms/step - loss: 0.3587 - dice_loss: 0.1178 - accuracy: 0.9445\n",
            "Epoch 37/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3679 - dice_loss: 0.1205 - accuracy: 0.9407\n",
            "Epoch 37: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 170ms/step - loss: 0.3679 - dice_loss: 0.1205 - accuracy: 0.9407\n",
            "Epoch 38/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3632 - dice_loss: 0.1193 - accuracy: 0.9431\n",
            "Epoch 38: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 170ms/step - loss: 0.3632 - dice_loss: 0.1193 - accuracy: 0.9431\n",
            "Epoch 39/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3624 - dice_loss: 0.1189 - accuracy: 0.9427\n",
            "Epoch 39: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 170ms/step - loss: 0.3624 - dice_loss: 0.1189 - accuracy: 0.9427\n",
            "Epoch 40/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3656 - dice_loss: 0.1200 - accuracy: 0.9420\n",
            "Epoch 40: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 170ms/step - loss: 0.3656 - dice_loss: 0.1200 - accuracy: 0.9420\n",
            "Epoch 41/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3655 - dice_loss: 0.1200 - accuracy: 0.9426\n",
            "Epoch 41: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3655 - dice_loss: 0.1200 - accuracy: 0.9426\n",
            "Epoch 42/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3615 - dice_loss: 0.1188 - accuracy: 0.9441\n",
            "Epoch 42: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3615 - dice_loss: 0.1188 - accuracy: 0.9441\n",
            "Epoch 43/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3631 - dice_loss: 0.1192 - accuracy: 0.9428\n",
            "Epoch 43: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 168ms/step - loss: 0.3631 - dice_loss: 0.1192 - accuracy: 0.9428\n",
            "Epoch 44/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3671 - dice_loss: 0.1205 - accuracy: 0.9418\n",
            "Epoch 44: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3671 - dice_loss: 0.1205 - accuracy: 0.9418\n",
            "Epoch 45/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3624 - dice_loss: 0.1191 - accuracy: 0.9435\n",
            "Epoch 45: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 168ms/step - loss: 0.3624 - dice_loss: 0.1191 - accuracy: 0.9435\n",
            "Epoch 46/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3605 - dice_loss: 0.1185 - accuracy: 0.9440\n",
            "Epoch 46: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3605 - dice_loss: 0.1185 - accuracy: 0.9440\n",
            "Epoch 47/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3622 - dice_loss: 0.1190 - accuracy: 0.9435\n",
            "Epoch 47: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 168ms/step - loss: 0.3622 - dice_loss: 0.1190 - accuracy: 0.9435\n",
            "Epoch 48/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3610 - dice_loss: 0.1187 - accuracy: 0.9446\n",
            "Epoch 48: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3610 - dice_loss: 0.1187 - accuracy: 0.9446\n",
            "Epoch 49/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3612 - dice_loss: 0.1187 - accuracy: 0.9440\n",
            "Epoch 49: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 168ms/step - loss: 0.3612 - dice_loss: 0.1187 - accuracy: 0.9440\n",
            "Epoch 50/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3661 - dice_loss: 0.1202 - accuracy: 0.9428\n",
            "Epoch 50: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 168ms/step - loss: 0.3661 - dice_loss: 0.1202 - accuracy: 0.9428\n",
            "Epoch 51/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3616 - dice_loss: 0.1188 - accuracy: 0.9442\n",
            "Epoch 51: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3616 - dice_loss: 0.1188 - accuracy: 0.9442\n",
            "Epoch 52/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3591 - dice_loss: 0.1181 - accuracy: 0.9448\n",
            "Epoch 52: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3591 - dice_loss: 0.1181 - accuracy: 0.9448\n",
            "Epoch 53/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3638 - dice_loss: 0.1195 - accuracy: 0.9430\n",
            "Epoch 53: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 168ms/step - loss: 0.3638 - dice_loss: 0.1195 - accuracy: 0.9430\n",
            "Epoch 54/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3614 - dice_loss: 0.1187 - accuracy: 0.9444\n",
            "Epoch 54: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 168ms/step - loss: 0.3614 - dice_loss: 0.1187 - accuracy: 0.9444\n",
            "Epoch 55/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3642 - dice_loss: 0.1197 - accuracy: 0.9435\n",
            "Epoch 55: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3642 - dice_loss: 0.1197 - accuracy: 0.9435\n",
            "Epoch 56/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3639 - dice_loss: 0.1196 - accuracy: 0.9442\n",
            "Epoch 56: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 170ms/step - loss: 0.3639 - dice_loss: 0.1196 - accuracy: 0.9442\n",
            "Epoch 57/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3581 - dice_loss: 0.1179 - accuracy: 0.9457\n",
            "Epoch 57: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3581 - dice_loss: 0.1179 - accuracy: 0.9457\n",
            "Epoch 58/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3640 - dice_loss: 0.1196 - accuracy: 0.9434\n",
            "Epoch 58: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3640 - dice_loss: 0.1196 - accuracy: 0.9434\n",
            "Epoch 59/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3647 - dice_loss: 0.1197 - accuracy: 0.9429\n",
            "Epoch 59: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3647 - dice_loss: 0.1197 - accuracy: 0.9429\n",
            "Epoch 60/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3605 - dice_loss: 0.1186 - accuracy: 0.9454\n",
            "Epoch 60: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3605 - dice_loss: 0.1186 - accuracy: 0.9454\n",
            "Epoch 61/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3621 - dice_loss: 0.1191 - accuracy: 0.9446\n",
            "Epoch 61: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 170ms/step - loss: 0.3621 - dice_loss: 0.1191 - accuracy: 0.9446\n",
            "Epoch 62/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3554 - dice_loss: 0.1171 - accuracy: 0.9472\n",
            "Epoch 62: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 170ms/step - loss: 0.3554 - dice_loss: 0.1171 - accuracy: 0.9472\n",
            "Epoch 63/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3627 - dice_loss: 0.1192 - accuracy: 0.9444\n",
            "Epoch 63: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 170ms/step - loss: 0.3627 - dice_loss: 0.1192 - accuracy: 0.9444\n",
            "Epoch 64/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3634 - dice_loss: 0.1194 - accuracy: 0.9438\n",
            "Epoch 64: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3634 - dice_loss: 0.1194 - accuracy: 0.9438\n",
            "Epoch 65/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3578 - dice_loss: 0.1178 - accuracy: 0.9463\n",
            "Epoch 65: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 170ms/step - loss: 0.3578 - dice_loss: 0.1178 - accuracy: 0.9463\n",
            "Epoch 66/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3625 - dice_loss: 0.1192 - accuracy: 0.9443\n",
            "Epoch 66: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3625 - dice_loss: 0.1192 - accuracy: 0.9443\n",
            "Epoch 67/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3643 - dice_loss: 0.1197 - accuracy: 0.9437\n",
            "Epoch 67: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3643 - dice_loss: 0.1197 - accuracy: 0.9437\n",
            "Epoch 68/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3644 - dice_loss: 0.1196 - accuracy: 0.9435\n",
            "Epoch 68: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3644 - dice_loss: 0.1196 - accuracy: 0.9435\n",
            "Epoch 69/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3588 - dice_loss: 0.1181 - accuracy: 0.9460\n",
            "Epoch 69: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 168ms/step - loss: 0.3588 - dice_loss: 0.1181 - accuracy: 0.9460\n",
            "Epoch 70/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3628 - dice_loss: 0.1191 - accuracy: 0.9435\n",
            "Epoch 70: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 170ms/step - loss: 0.3628 - dice_loss: 0.1191 - accuracy: 0.9435\n",
            "Epoch 71/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3641 - dice_loss: 0.1197 - accuracy: 0.9442\n",
            "Epoch 71: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 168ms/step - loss: 0.3641 - dice_loss: 0.1197 - accuracy: 0.9442\n",
            "Epoch 72/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3600 - dice_loss: 0.1184 - accuracy: 0.9450\n",
            "Epoch 72: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 170ms/step - loss: 0.3600 - dice_loss: 0.1184 - accuracy: 0.9450\n",
            "Epoch 73/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3617 - dice_loss: 0.1189 - accuracy: 0.9447\n",
            "Epoch 73: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3617 - dice_loss: 0.1189 - accuracy: 0.9447\n",
            "Epoch 74/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3603 - dice_loss: 0.1185 - accuracy: 0.9456\n",
            "Epoch 74: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 170ms/step - loss: 0.3603 - dice_loss: 0.1185 - accuracy: 0.9456\n",
            "Epoch 75/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3633 - dice_loss: 0.1195 - accuracy: 0.9450\n",
            "Epoch 75: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3633 - dice_loss: 0.1195 - accuracy: 0.9450\n",
            "Epoch 76/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3611 - dice_loss: 0.1188 - accuracy: 0.9453\n",
            "Epoch 76: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3611 - dice_loss: 0.1188 - accuracy: 0.9453\n",
            "Epoch 77/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3614 - dice_loss: 0.1189 - accuracy: 0.9454\n",
            "Epoch 77: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 168ms/step - loss: 0.3614 - dice_loss: 0.1189 - accuracy: 0.9454\n",
            "Epoch 78/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3608 - dice_loss: 0.1188 - accuracy: 0.9456\n",
            "Epoch 78: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 168ms/step - loss: 0.3608 - dice_loss: 0.1188 - accuracy: 0.9456\n",
            "Epoch 79/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3611 - dice_loss: 0.1188 - accuracy: 0.9454\n",
            "Epoch 79: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3611 - dice_loss: 0.1188 - accuracy: 0.9454\n",
            "Epoch 80/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3612 - dice_loss: 0.1190 - accuracy: 0.9462\n",
            "Epoch 80: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 170ms/step - loss: 0.3612 - dice_loss: 0.1190 - accuracy: 0.9462\n",
            "Epoch 81/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3565 - dice_loss: 0.1174 - accuracy: 0.9475\n",
            "Epoch 81: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3565 - dice_loss: 0.1174 - accuracy: 0.9475\n",
            "Epoch 82/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3618 - dice_loss: 0.1191 - accuracy: 0.9450\n",
            "Epoch 82: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 168ms/step - loss: 0.3618 - dice_loss: 0.1191 - accuracy: 0.9450\n",
            "Epoch 83/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3616 - dice_loss: 0.1190 - accuracy: 0.9458\n",
            "Epoch 83: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 170ms/step - loss: 0.3616 - dice_loss: 0.1190 - accuracy: 0.9458\n",
            "Epoch 84/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3572 - dice_loss: 0.1177 - accuracy: 0.9473\n",
            "Epoch 84: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3572 - dice_loss: 0.1177 - accuracy: 0.9473\n",
            "Epoch 85/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3614 - dice_loss: 0.1189 - accuracy: 0.9462\n",
            "Epoch 85: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3614 - dice_loss: 0.1189 - accuracy: 0.9462\n",
            "Epoch 86/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3597 - dice_loss: 0.1185 - accuracy: 0.9468\n",
            "Epoch 86: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3597 - dice_loss: 0.1185 - accuracy: 0.9468\n",
            "Epoch 87/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3604 - dice_loss: 0.1186 - accuracy: 0.9464\n",
            "Epoch 87: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 170ms/step - loss: 0.3604 - dice_loss: 0.1186 - accuracy: 0.9464\n",
            "Epoch 88/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3602 - dice_loss: 0.1184 - accuracy: 0.9455\n",
            "Epoch 88: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3602 - dice_loss: 0.1184 - accuracy: 0.9455\n",
            "Epoch 89/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3620 - dice_loss: 0.1191 - accuracy: 0.9447\n",
            "Epoch 89: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3620 - dice_loss: 0.1191 - accuracy: 0.9447\n",
            "Epoch 90/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3580 - dice_loss: 0.1178 - accuracy: 0.9467\n",
            "Epoch 90: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 170ms/step - loss: 0.3580 - dice_loss: 0.1178 - accuracy: 0.9467\n",
            "Epoch 91/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3610 - dice_loss: 0.1188 - accuracy: 0.9460\n",
            "Epoch 91: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 170ms/step - loss: 0.3610 - dice_loss: 0.1188 - accuracy: 0.9460\n",
            "Epoch 92/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3602 - dice_loss: 0.1186 - accuracy: 0.9463\n",
            "Epoch 92: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 170ms/step - loss: 0.3602 - dice_loss: 0.1186 - accuracy: 0.9463\n",
            "Epoch 93/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3590 - dice_loss: 0.1183 - accuracy: 0.9474\n",
            "Epoch 93: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 170ms/step - loss: 0.3590 - dice_loss: 0.1183 - accuracy: 0.9474\n",
            "Epoch 94/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3621 - dice_loss: 0.1191 - accuracy: 0.9451\n",
            "Epoch 94: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3621 - dice_loss: 0.1191 - accuracy: 0.9451\n",
            "Epoch 95/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3577 - dice_loss: 0.1178 - accuracy: 0.9474\n",
            "Epoch 95: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 170ms/step - loss: 0.3577 - dice_loss: 0.1178 - accuracy: 0.9474\n",
            "Epoch 96/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3675 - dice_loss: 0.1208 - accuracy: 0.9441\n",
            "Epoch 96: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 170ms/step - loss: 0.3675 - dice_loss: 0.1208 - accuracy: 0.9441\n",
            "Epoch 97/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3553 - dice_loss: 0.1172 - accuracy: 0.9487\n",
            "Epoch 97: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3553 - dice_loss: 0.1172 - accuracy: 0.9487\n",
            "Epoch 98/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3594 - dice_loss: 0.1184 - accuracy: 0.9470\n",
            "Epoch 98: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3594 - dice_loss: 0.1184 - accuracy: 0.9470\n",
            "Epoch 99/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3625 - dice_loss: 0.1192 - accuracy: 0.9454\n",
            "Epoch 99: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 169ms/step - loss: 0.3625 - dice_loss: 0.1192 - accuracy: 0.9454\n",
            "Epoch 100/100\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.3550 - dice_loss: 0.1172 - accuracy: 0.9494\n",
            "Epoch 100: loss did not improve from -0.77503\n",
            "20/20 [==============================] - 3s 168ms/step - loss: 0.3550 - dice_loss: 0.1172 - accuracy: 0.9494\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7e2c20a3d0>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Losses"
      ],
      "metadata": {
        "id": "1Ttjh-QQEGVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''# Dice similarity coefficient loss, brought to you by: https://github.com/nabsabraham/focal-tversky-unet\n",
        "from tensorflow import reduce_sum\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "\n",
        "def dsc(y_true, y_pred):\n",
        "    smooth = 1.\n",
        "    y_true_f = Flatten()(y_true)\n",
        "    y_pred_f = Flatten()(y_pred)\n",
        "    intersection = reduce_sum(y_true_f * y_pred_f)\n",
        "    score = (2. * intersection + smooth) / (reduce_sum(y_true_f) + reduce_sum(y_pred_f) + smooth)\n",
        "    return score\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "    loss = 1 - dsc(y_true, y_pred)\n",
        "    return loss\n",
        "\n",
        "def bce_dice_loss(y_true, y_pred):\n",
        "    loss = binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
        "    return loss'''"
      ],
      "metadata": {
        "id": "aLSjtSCkEFr9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "5ab31e09-e2ee-4c95-dace-4c216916f8b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# Dice similarity coefficient loss, brought to you by: https://github.com/nabsabraham/focal-tversky-unet\\nfrom tensorflow import reduce_sum\\nfrom tensorflow.keras.losses import binary_crossentropy\\n\\ndef dsc(y_true, y_pred):\\n    smooth = 1.\\n    y_true_f = Flatten()(y_true)\\n    y_pred_f = Flatten()(y_pred)\\n    intersection = reduce_sum(y_true_f * y_pred_f)\\n    score = (2. * intersection + smooth) / (reduce_sum(y_true_f) + reduce_sum(y_pred_f) + smooth)\\n    return score\\n\\ndef dice_loss(y_true, y_pred):\\n    loss = 1 - dsc(y_true, y_pred)\\n    return loss\\n\\ndef bce_dice_loss(y_true, y_pred):\\n    loss = binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\\n    return loss'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "\n",
        "data_gen_args = dict(rotation_range=0.2,\n",
        "                    width_shift_range=0.05,\n",
        "                    height_shift_range=0.05,\n",
        "                    shear_range=0.05,\n",
        "                    zoom_range=0.05,\n",
        "                    horizontal_flip=True,\n",
        "                    fill_mode='nearest')\n",
        "myGene = trainGenerator(2,\"/content/drive/MyDrive/membrane/train\",'image','label',data_gen_args,save_to_dir = None)\n",
        "\n",
        "modelnn = Nest_Net()\n",
        "modelnn.summary()\n",
        "\n",
        "\n",
        "adam = Adam(lr = 0.05, epsilon = 0.1)\n",
        "modelnn.compile(optimizer=adam, loss=bce_dice_loss, metrics=[dice_loss])\n",
        "\n",
        "\n",
        "model_checkpoint = ModelCheckpoint('nest_net_membrane.hdf5', monitor='loss',verbose=1, save_best_only=True)\n",
        "modelnn.fit_generator(myGene,steps_per_epoch=30,epochs=1,callbacks=[model_checkpoint])\n",
        "\n",
        "testGene = testGenerator(\"/content/drive/MyDrive/membrane/test\")\n",
        "results = model.predict_generator(testGene,30,verbose=1)\n",
        "#saveResult(\"/content/drive/MyDrive/membrane/test\",results)'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "cmF4khHSCbzO",
        "outputId": "9489a609-6cc9-48a4-a2f1-d4049c64d59d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\\n\\n\\ndata_gen_args = dict(rotation_range=0.2,\\n                    width_shift_range=0.05,\\n                    height_shift_range=0.05,\\n                    shear_range=0.05,\\n                    zoom_range=0.05,\\n                    horizontal_flip=True,\\n                    fill_mode=\\'nearest\\')\\nmyGene = trainGenerator(2,\"/content/drive/MyDrive/membrane/train\",\\'image\\',\\'label\\',data_gen_args,save_to_dir = None)\\n\\nmodelnn = Nest_Net()\\nmodelnn.summary()\\n\\n\\nadam = Adam(lr = 0.05, epsilon = 0.1)\\nmodelnn.compile(optimizer=adam, loss=bce_dice_loss, metrics=[dice_loss])\\n\\n\\nmodel_checkpoint = ModelCheckpoint(\\'nest_net_membrane.hdf5\\', monitor=\\'loss\\',verbose=1, save_best_only=True)\\nmodelnn.fit_generator(myGene,steps_per_epoch=30,epochs=1,callbacks=[model_checkpoint])\\n\\ntestGene = testGenerator(\"/content/drive/MyDrive/membrane/test\")\\nresults = model.predict_generator(testGene,30,verbose=1)\\n#saveResult(\"/content/drive/MyDrive/membrane/test\",results)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testGene = testGenerator(\"/content/drive/MyDrive/membrane/test\")\n",
        "print(testGene)"
      ],
      "metadata": {
        "id": "YqOOv7y5NMtx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34ebbc40-97f5-48b3-d029-52bba47c2ca9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<generator object testGenerator at 0x7f7dee40bd50>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.predict_generator(testGene,30,verbose=1)"
      ],
      "metadata": {
        "id": "hoSpKVewNazt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fc8cfb4-be79-4d0a-e8f4-00aaacad8e6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30/30 [==============================] - 8s 276ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model.fit_generator(myGene,steps_per_epoch=20,epochs=5,callbacks=[model_checkpoint])"
      ],
      "metadata": {
        "id": "CwBR0U7FOKqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(results)"
      ],
      "metadata": {
        "id": "tn3zLNXQOxSz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c893ee0-f145-479e-9e72-78b221d6c8ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[[0.90142936]\n",
            "   [0.9075146 ]\n",
            "   [0.90561825]\n",
            "   ...\n",
            "   [0.9042666 ]\n",
            "   [0.90175796]\n",
            "   [0.88767684]]\n",
            "\n",
            "  [[0.90132236]\n",
            "   [0.90387994]\n",
            "   [0.9023833 ]\n",
            "   ...\n",
            "   [0.903818  ]\n",
            "   [0.9012464 ]\n",
            "   [0.8849307 ]]\n",
            "\n",
            "  [[0.9032344 ]\n",
            "   [0.90464455]\n",
            "   [0.9036386 ]\n",
            "   ...\n",
            "   [0.9031565 ]\n",
            "   [0.8860696 ]\n",
            "   [0.8126805 ]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.8991604 ]\n",
            "   [0.902323  ]\n",
            "   [0.90088135]\n",
            "   ...\n",
            "   [0.9058507 ]\n",
            "   [0.9031259 ]\n",
            "   [0.9059054 ]]\n",
            "\n",
            "  [[0.8913739 ]\n",
            "   [0.89871275]\n",
            "   [0.9016522 ]\n",
            "   ...\n",
            "   [0.904329  ]\n",
            "   [0.9037035 ]\n",
            "   [0.90551776]]\n",
            "\n",
            "  [[0.8623498 ]\n",
            "   [0.8931746 ]\n",
            "   [0.8978137 ]\n",
            "   ...\n",
            "   [0.9019772 ]\n",
            "   [0.9026525 ]\n",
            "   [0.900879  ]]]\n",
            "\n",
            "\n",
            " [[[0.8997155 ]\n",
            "   [0.90700275]\n",
            "   [0.90543795]\n",
            "   ...\n",
            "   [0.41946915]\n",
            "   [0.41788834]\n",
            "   [0.5643392 ]]\n",
            "\n",
            "  [[0.9003645 ]\n",
            "   [0.9038842 ]\n",
            "   [0.9017985 ]\n",
            "   ...\n",
            "   [0.42327997]\n",
            "   [0.52265286]\n",
            "   [0.7528617 ]]\n",
            "\n",
            "  [[0.9012798 ]\n",
            "   [0.9027299 ]\n",
            "   [0.9051924 ]\n",
            "   ...\n",
            "   [0.7968201 ]\n",
            "   [0.88966805]\n",
            "   [0.89732206]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.9003276 ]\n",
            "   [0.90257037]\n",
            "   [0.9031287 ]\n",
            "   ...\n",
            "   [0.9026535 ]\n",
            "   [0.9054701 ]\n",
            "   [0.9035718 ]]\n",
            "\n",
            "  [[0.90165824]\n",
            "   [0.9025956 ]\n",
            "   [0.9044847 ]\n",
            "   ...\n",
            "   [0.9032492 ]\n",
            "   [0.90521437]\n",
            "   [0.9017584 ]]\n",
            "\n",
            "  [[0.90168816]\n",
            "   [0.90274984]\n",
            "   [0.90191853]\n",
            "   ...\n",
            "   [0.90325767]\n",
            "   [0.9008062 ]\n",
            "   [0.89206433]]]\n",
            "\n",
            "\n",
            " [[[0.9030936 ]\n",
            "   [0.90406346]\n",
            "   [0.90353084]\n",
            "   ...\n",
            "   [0.87373406]\n",
            "   [0.8853213 ]\n",
            "   [0.8861677 ]]\n",
            "\n",
            "  [[0.8998492 ]\n",
            "   [0.9035221 ]\n",
            "   [0.90398663]\n",
            "   ...\n",
            "   [0.895858  ]\n",
            "   [0.8990971 ]\n",
            "   [0.88461363]]\n",
            "\n",
            "  [[0.90392464]\n",
            "   [0.9041472 ]\n",
            "   [0.9042108 ]\n",
            "   ...\n",
            "   [0.90166336]\n",
            "   [0.90228474]\n",
            "   [0.8861478 ]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.89338577]\n",
            "   [0.9003652 ]\n",
            "   [0.9020414 ]\n",
            "   ...\n",
            "   [0.6512173 ]\n",
            "   [0.69882154]\n",
            "   [0.7225793 ]]\n",
            "\n",
            "  [[0.8032353 ]\n",
            "   [0.8595895 ]\n",
            "   [0.8591694 ]\n",
            "   ...\n",
            "   [0.89839566]\n",
            "   [0.90273345]\n",
            "   [0.8873405 ]]\n",
            "\n",
            "  [[0.67982537]\n",
            "   [0.6945882 ]\n",
            "   [0.6527631 ]\n",
            "   ...\n",
            "   [0.9066424 ]\n",
            "   [0.9018844 ]\n",
            "   [0.8848748 ]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[0.9021649 ]\n",
            "   [0.905423  ]\n",
            "   [0.9032389 ]\n",
            "   ...\n",
            "   [0.90325165]\n",
            "   [0.84550315]\n",
            "   [0.782882  ]]\n",
            "\n",
            "  [[0.9008171 ]\n",
            "   [0.90450704]\n",
            "   [0.90399647]\n",
            "   ...\n",
            "   [0.903221  ]\n",
            "   [0.8428946 ]\n",
            "   [0.72320664]]\n",
            "\n",
            "  [[0.90197164]\n",
            "   [0.9039233 ]\n",
            "   [0.90330523]\n",
            "   ...\n",
            "   [0.882509  ]\n",
            "   [0.67369777]\n",
            "   [0.54626065]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.4498272 ]\n",
            "   [0.68383354]\n",
            "   [0.8993576 ]\n",
            "   ...\n",
            "   [0.8934469 ]\n",
            "   [0.87104106]\n",
            "   [0.80429184]]\n",
            "\n",
            "  [[0.488711  ]\n",
            "   [0.7174111 ]\n",
            "   [0.8996944 ]\n",
            "   ...\n",
            "   [0.9041399 ]\n",
            "   [0.90737593]\n",
            "   [0.88732076]]\n",
            "\n",
            "  [[0.5615824 ]\n",
            "   [0.70034903]\n",
            "   [0.88096935]\n",
            "   ...\n",
            "   [0.89885235]\n",
            "   [0.896169  ]\n",
            "   [0.8645076 ]]]\n",
            "\n",
            "\n",
            " [[[0.902685  ]\n",
            "   [0.90346265]\n",
            "   [0.9033026 ]\n",
            "   ...\n",
            "   [0.8437785 ]\n",
            "   [0.87141114]\n",
            "   [0.8776126 ]]\n",
            "\n",
            "  [[0.8987963 ]\n",
            "   [0.90383095]\n",
            "   [0.90425277]\n",
            "   ...\n",
            "   [0.90217817]\n",
            "   [0.89862233]\n",
            "   [0.89440924]]\n",
            "\n",
            "  [[0.8987283 ]\n",
            "   [0.90410185]\n",
            "   [0.90347207]\n",
            "   ...\n",
            "   [0.9021007 ]\n",
            "   [0.90202737]\n",
            "   [0.8959015 ]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.90382   ]\n",
            "   [0.8230304 ]\n",
            "   [0.7237091 ]\n",
            "   ...\n",
            "   [0.90329343]\n",
            "   [0.9023842 ]\n",
            "   [0.90609485]]\n",
            "\n",
            "  [[0.90292037]\n",
            "   [0.837952  ]\n",
            "   [0.6889833 ]\n",
            "   ...\n",
            "   [0.90207756]\n",
            "   [0.9003777 ]\n",
            "   [0.9044726 ]]\n",
            "\n",
            "  [[0.88772154]\n",
            "   [0.82501084]\n",
            "   [0.6771684 ]\n",
            "   ...\n",
            "   [0.9076082 ]\n",
            "   [0.9034002 ]\n",
            "   [0.89918023]]]\n",
            "\n",
            "\n",
            " [[[0.87713426]\n",
            "   [0.9081222 ]\n",
            "   [0.9094382 ]\n",
            "   ...\n",
            "   [0.90465003]\n",
            "   [0.9022591 ]\n",
            "   [0.90497094]]\n",
            "\n",
            "  [[0.8448985 ]\n",
            "   [0.9072988 ]\n",
            "   [0.9056416 ]\n",
            "   ...\n",
            "   [0.9044136 ]\n",
            "   [0.90258706]\n",
            "   [0.9044752 ]]\n",
            "\n",
            "  [[0.84615356]\n",
            "   [0.89560205]\n",
            "   [0.90148264]\n",
            "   ...\n",
            "   [0.902408  ]\n",
            "   [0.90365493]\n",
            "   [0.90468854]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.759571  ]\n",
            "   [0.5815517 ]\n",
            "   [0.44063565]\n",
            "   ...\n",
            "   [0.90339607]\n",
            "   [0.9026708 ]\n",
            "   [0.9061765 ]]\n",
            "\n",
            "  [[0.88710606]\n",
            "   [0.8529635 ]\n",
            "   [0.6828938 ]\n",
            "   ...\n",
            "   [0.90451056]\n",
            "   [0.9030974 ]\n",
            "   [0.90516543]]\n",
            "\n",
            "  [[0.87067556]\n",
            "   [0.8460402 ]\n",
            "   [0.8020633 ]\n",
            "   ...\n",
            "   [0.905449  ]\n",
            "   [0.9040697 ]\n",
            "   [0.90120685]]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#history = model.fit_generator(myGene,steps_per_epoch=20,epochs=5,callbacks=[model_checkpoint])"
      ],
      "metadata": {
        "id": "v4LHz-TwpAzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plots"
      ],
      "metadata": {
        "id": "hCK5vbnsZ1yJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pylab as plt"
      ],
      "metadata": {
        "id": "KZUAbh3fo414"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,5))\n",
        "#plt.subplot(1,2,1)\n",
        "#plt.plot(history_dice.history['epochs'])\n",
        "plt.plot(history_dice.history['dice_loss'],color = 'Red')\n",
        "plt.title('dice_loss vs epoch')\n",
        "plt.ylabel('dice_loss')\n",
        "plt.xlabel('epoch')"
      ],
      "metadata": {
        "id": "E0kO6pzlov3M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "9c402f99-80b6-411c-e1d6-81d4e741b2f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'epoch')"
            ]
          },
          "metadata": {},
          "execution_count": 29
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize history for accuracy\n",
        "plt.figure(figsize=(20,5))\n",
        "#plt.subplot(1,2,1)\n",
        "#plt.plot(history_dice.history['epochs'])\n",
        "plt.plot(history_jaccard.history['jacard_coef'], color = 'Red')\n",
        "plt.title('Jaccard Coefficient vs epoch')\n",
        "plt.ylabel('jacard_coef')\n",
        "plt.xlabel('epoch')"
      ],
      "metadata": {
        "id": "Alf8xVsFo0Sg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "58b7787e-792c-4218-ee1d-9205caaa36af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'epoch')"
            ]
          },
          "metadata": {},
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,5))\n",
        "#plt.subplot(1,2,1)\n",
        "#plt.plot(history_dice.history['epochs'])\n",
        "plt.plot(history_dice.history['accuracy'], color = 'Red')\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('epoch')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "admi8bs7w-xv",
        "outputId": "190eea96-1617-4aa3-ab1a-92069e6c8d40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'epoch')"
            ]
          },
          "metadata": {},
          "execution_count": 33
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bCRSqLRQ22vc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}